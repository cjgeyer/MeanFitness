
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage[colorlinks=true,allcolors=blue]{hyperref}
\usepackage{doi}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}

\let\code=\texttt

\newcommand{\REVISED}{\begin{center} \LARGE REVISED DOWN TO HERE \end{center}}
\newcommand{\MOVED}[1][equation]{\begin{center} [#1 moved] \end{center}}

\begin{document}

\title{Estimating Population Mean Fitness and Change Thereof When
    the Population Distribution is Unknown}

\author{Charles J. Geyer \and Ruth G. Shaw}

\maketitle

\section{Estimates}

\subsection{Experiment}

We have an experiment in which (Darwinian) fitness (measured as lifetime
number of offspring per individual) was measured under wild conditions.
The individuals in the experiment were not a random sample from
a population but rather offspring from crosses in the experimental design having distinct sets of maternal
individuals mated to individual paternal ones, that is,
generating full-sib groups nested within paternal half-sib
groups.  There are no data on
population allele frequencies, and even if there were, there would be
no information of the bearing of the alleles on fitness of individuals in the population. 
Thus we simply assume some population distribution (of
pedigreed crosses).  All of our inferences will be sensitive to this
assumption, but we can check how sensitive (by trying different assumptions
or doing other mathematical sensitivity analysis).

The particular experiment at hand has five subexperiments laid out as follows.
\begin{center}
\begin{tabular}{ccc}
year one & year two & year three \\
\hline
parents & parents & parents \\
        & offspring & offspring
\end{tabular}
\end{center}
Here ``parents'' means individuals produced by the experimental crosses (seeds from
the same crosses were planted in each year).  For short, we will say these
individuals are from the same ``family'' if they are known sibs
(that is, have the same sire in the experimental crosses).
Here ``offspring'' means individuals whose dam was a ``parent'' from the
previous year (their own sire unknown because of open pollination).
For short, we will say these
individuals are from the same ``family'' as their dam in the previous year.

\subsection{Change in Mean Fitness}

If
\begin{itemize}
\item $p_i$ is the (assumed) population frequency of family $i$
    (perhaps assumed to be equal for all $i$, perhaps not), and
\item $\hat{\mu}_i$ is (estimated) mean fitness for family $i$,
\end{itemize}
then
\begin{itemize}
\item $\sum_i p_i \hat{\mu}_i$ is (estimated) mean fitness for
that subexperiment, where the sum runs over all families.
\end{itemize}

We also need to know the (estimated) population frequency
\emph{after selection}, which is
$$
   \hat{q}_i = \frac{p_i \hat{\mu}_i}{\sum_j p_j \hat{\mu}_j}
$$
where the sum in the denominator runs over all families.
(This is standard in quantitative genetics, \citet{lande-arnold},
breeder's equation,
and so forth, but does not account for recombination and other effects.)

Now we need to consider three subexperiments
\begin{itemize}
\item a ``parent'' subexperiment in one year (denoted by superscript prev),
\item an ``offspring'' subexperiment in the next year (denoted
    by superscript off), and
\item the ``parent'' subexperiment in the same year as the ``offspring
    experiment (denoted by superscript par).
\end{itemize}
(There are two such triangles of subexperiments in these data, one
for each offspring year.)

The total change in mean fitness in from one generation to (the same point in) the next is
\begin{equation} \label{eq:total-change}
   \sum\nolimits_i \bigl(
   \hat{q}_i^\text{prev} \hat{\mu}_i^\text{off}
   -
   p_i^\text{prev} \hat{\mu}_i^\text{prev}
   \bigr)
\end{equation}

\subsection{Decomposition of Change in Mean Fitness}

We now decompose \eqref{eq:total-change} into a sum of three terms,
each of which has a biological interpretation.

First we look at the change in mean fitness due to change in environment
(between the two years involved in this analysis), which we take to be
\begin{subequations}
\begin{equation} \label{eq:partial-change-environmental}
   \sum\nolimits_i p_i^\text{prev}
   \bigl( \hat{\mu}_i^\text{par} - \hat{\mu}_i^\text{prev} \bigr)
\end{equation}

Subtracting \eqref{eq:partial-change-environmental} from \eqref{eq:total-change}
gives
$$
   \sum\nolimits_i \bigl(
   \hat{q}_i^\text{prev} \hat{\mu}_i^\text{off}
   -
   p_i^\text{prev} \hat{\mu}_i^\text{par}
   \bigr)
$$
which we take to be the change in fitness due to genetics (we are now
comparing parents and offspring in the same environment.

Fisher's fundamental theorem of natural selection (FFTNS) does not try
to predict all change in mean fitness but only that
due to natural selection'' which is often taken to be
\begin{equation} \label{eq:partial-change-fftns}
   \sum\nolimits_i
   \bigl(\hat{q}_i^\text{prev} - p_i^\text{prev}\bigr) \hat{\mu}_i^\text{par}
\end{equation}

Subtracting \eqref{eq:partial-change-fftns} from the unnumbered equation
preceding it gives
\begin{equation} \label{eq:partial-change-genetic-non-fftns}
    \sum\nolimits_i \hat{q}_i^\text{prev} \bigl(\hat{\mu}_i^\text{off} -
    \hat{\mu}_i^\text{par}\bigr)
\end{equation}
which we take to be the part of change in mean fitness that is genetic
but not predicted by FFTNS.
\end{subequations}

It is easily checked that
\eqref{eq:partial-change-environmental} plus
\eqref{eq:partial-change-fftns} plus
\eqref{eq:partial-change-genetic-non-fftns} gives
\eqref{eq:total-change}.  Thus we have decomposed the total change
in mean fitness in one generation into three parts, one of which is
the part of the change addressed by FFTNS and the
others have biological interpretations.

We intend to estimate, discuss, and interpret each of these three differences
separately.  We also need to derive standard errors for each.

\section{Standard Errors}

\subsection{Introduction}

In aster models with random effects \citep*{reaster} we have fixed effects
and random effects.  Fixed effects are unknown parameters to estimate.
Random effects are not.  They are random variables whose variances are
unknown parameters to estimate (so-called \emph{variance components}).

R function \code{reaster} in R package \code{aster} \citep{aster-package}
provides standard errors for the estimated parameters (fixed effects and
variance components) and, more generally, provides (an approximation to)
the Fisher information matrix for these parameters.  The latter is
undocumented although is used in published examples
\citep[Supplementary material Section~12.3.1]{zenodo}.  And the inverse
of this matrix is (by the ``usual'' asymptotics of maximum likelihood)
the variance-covariance matrix of the parameter estimates.

R function \code{reaster} also provides ``estimates'' (in scare quotes)
of random effects.  Of course, random effects are not constants to estimate,
so these are not like parameter estimates.  They estimate \emph{something}, but
what?

In classical quantitative genetics where all random variables in the model
(response and random effects) are jointly multivariate normal, one has
BLUP (best linear unbiased predictors) of the random effects.  These
are conditional expectations of random effects given the observed value
of the response (quantitative trait), and these are linear functions
of the response vector (because conditional expectations of multivariate
normal are linear) and unbiased (also a property of multivariate normal)
and best in the sense of having smaller variance than any other linear
unbiased predictions.

One can think of R function \code{reaster}'s ``estimates'' (in scare quotes)
of random effects as being analogs of BLUP's except without the B, L, and U.
Because the response in an aster model is not taken to be normally distributed,
the conditional distribution of random effects given observed response is
not normally distributed (and has no formula so could only be calculated
using Markov chain Monte Carlo (MCMC)).  Hence we lose the linearity (L),
unbiasedness (U), and best (B), which came from multivariate normality.
We also lose conditional expectation (no letter), which BLUP are,
but \code{reaster} ``estimates'' (in scare quotes) of random effects are not.
(If we wanted conditional expectations, we would need MCMC to obtain them.)

So after all of that explanation of what these ``estimates'' (in scare quotes)
are not, what are they?  For that we dive into the middle of the theory in
\citet[Section~2]{reaster}.  There we have the denotations $\alpha$ for
the vector of fixed effects, $b$ for the vector of random effects,
and $\nu$ for the vector of variance components, and estimates thereof
denoted $\hat{\alpha}$, $\hat{b}$, and $\hat{\nu}$ that jointly minimize
\begin{equation} \label{eq:pee}
   p(\alpha, b, \nu)
   =
   - l(a + M \alpha + Z b)
   + \tfrac{1}{2} b^T D^{-1} b
   + \tfrac{1}{2} \log
   \det\bigl[ Z^T \widehat{W} Z D + I \bigr]
\end{equation}
which is equation (7) in \citet{reaster}, where
\begin{itemize}
\item $l$ is the log likelihood function for the saturated model
    unconditional canonical parameter vector $\varphi$,
\item $a$ is the offset vector,
\item $M$ is the model matrix for fixed effects,
\item $Z$ is the model matrix for random effects,
\item $D$ is the variance-covariance matrix for random effects,
    which is assumed to be diagonal with diagonal components that
    are equal to components of the parameter vector $\nu$,
\item $\widehat{W}$ is a symmetric positive definite matrix that is supposed
    to be close to $W(a + M \hat{\alpha} + Z \hat{b})$
    where $W(\varphi) = \var_\varphi(Y)$, that is, we pretend $\widehat{W}$
    is a known constant (matrix) where it is actually estimated, and
\item $I$ is the identity matrix of the appropriate dimension (number
    of random effects.
\end{itemize}

Since our vector of estimates $(\hat{\alpha}, \hat{b}, \hat{\nu})$ is
the joint minimzer of \eqref{eq:pee}, the asymptotic distribution follows
from the usual asymptotics of $M$-estimates.  The asymptotic distribution
should be normal centered at whatever values make the derivatives of
\eqref{eq:pee} unbiased estimating equations.

Those first derivatives are given in Section~1.7 of \citet{tr692}
\begin{subequations}
\begin{equation} \label{eq:p-alpha}
   p_\alpha(\alpha, b, \nu)
   =
   - M^T \bigl[ y - \mu(a + M \alpha + Z b) \bigr]
\end{equation}
\begin{equation} \label{eq:p-b}
   p_b(\alpha, b, \nu)
   =
   - Z^T \bigl[ y - \mu(a + M \alpha + Z b) \bigr] + D^{- 1} b
\end{equation}
and
\begin{equation} \label{eq:p-nu}
   p_{\nu_k}(\alpha, b, \nu)
   =
   - \tfrac{1}{2} b^T D^{- 1} E_k D^{- 1} b
   + \tfrac{1}{2} \tr \Bigl(
   \bigl[ Z^T \widehat{W} Z D + I \bigr]^{- 1}
   Z^T \widehat{W} Z E_k
   \Bigr)
\end{equation}
\end{subequations}
where $E_k = \partial D / \partial \nu_k$ is a matrix of the same dimensions
as $D$ that indicates which elements of the diagonal of $D$ are equal to
$\nu_k$ (by definition, not by accident) and where $\tr$ is the trace operator
that returns the trace of a matrix.

\begin{thebibliography}{}

\bibitem[Geyer(2021)]{aster-package}
Geyer, C.~J. (2021).
\newblock R package \texttt{aster}: Aster Models, version 1.1-2.
\newblock \url{https://cran.r-project.org/package=aster}.

\bibitem[Geyer, et al.(2022)Geyer, Kulbaba, Sheth, Pain, Eckhart,
    and Shaw]{zenodo}
Geyer, C.~J., Kulbaba, M.~W., Sheth, S.~N., Pain, R.~E., Eckhart, V.~M.,
    and Shaw, R.~G. (2022).
\newblock Correction for Kulbaba et al. (2019).
\newblock \emph{Evolution}, \textbf{76}, 3074.
\newblock \doi{10.1111/evo.14607}.
\newblock Supplementary material, version 2.0.1.
\newblock \doi{10.5281/zenodo.7013098}.

\bibitem[Geyer, et al.(2012)Geyer, Ridley, Latta, Etterson, and Shaw]{tr692}
Geyer, C.~J., Ridley, C.~E., Latta, R.~G., Etterson, J.~R and Shaw, R.~G.
    (2012).
\newblock Aster Models with Random Effects via Penalized Likelihood.
\newblock Technical Report No. 692.  School of Statistics,
    University of Minnesota.
\newblock \url{https://hdl.handle.net/11299/135870}.

\bibitem[Geyer, et al.(2013)Geyer, Ridley, Latta, Etterson, and Shaw]{reaster}
Geyer, C.~J., Ridley, C.~E., Latta, R.~G., Etterson, J.~R., and Shaw, R.~G.
    (2013).
\newblock Local Adaptation and genetic effects on fitness: Calculations for
    exponential family models with random effects.
\newblock \emph{Annals of Applied Statistics}, \textbf{7}, 1778--1795.
\newblock \doi{10.1214/13-AOAS653}.

\bibitem[Lande and Arnold(1983)]{lande-arnold}
Lande, R. and Arnold, S.~J. (1983).
\newblock The measurement of selection on correlated characters.
\newblock \emph{Evolution}, \textbf{37}, 1210--1226.

\bibitem[van der Vaart(1998)]{vdv}
van der Vaart, A.~W. (1998).
\newblock \emph{Asymptotic Statistics}.
\newblock Cambridge University Press.

\end{thebibliography}

\end{document}
