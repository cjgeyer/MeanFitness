
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage[colorlinks=true,allcolors=blue]{hyperref}
\usepackage{doi}

\newcommand{\fatdot}{\,\cdot\,}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}

\let\code=\texttt

\newcommand{\REVISED}{\begin{center} \LARGE REVISED DOWN TO HERE \end{center}}
\newcommand{\MOVED}[1][equation]{\begin{center} [#1 moved] \end{center}}

\begin{document}

\title{Decomposing Change in Population Mean Fitness}

\author{Charles J. Geyer \and Ruth G. Shaw}

\maketitle

\begin{abstract}
We previously measured mean fitness and additive genetic variance
under wild conditions in an annual plant \emph{Chamaecrista fasciculata}.
Here we analyze change in mean fitness across one generation and
decompose that into three parts: change predicted by Fisher's
fundamental theorem of natural selection, environmental change,
and the part of genetic change not addressed by Fisher's fundamental
theorem.  We also provide standard errors for these estimates.
\end{abstract}

\section{Estimates}

\subsection{Experiment}

We have an experiment \citep{kulbaba,zenodo} in which (Darwinian) fitness
(measured as lifetime
number of offspring per individual) was measured under wild conditions.
The individuals in the experiment were not a random sample from
a population but rather offspring from crosses in the experimental design having distinct sets of maternal
individuals mated to individual paternal ones, that is,
generating full-sib groups nested within paternal half-sib
groups.  There are no data on
population allele frequencies, and even if there were, there would be
no information of the bearing of the alleles on fitness of individuals in the population. 
Thus we simply assume some population distribution (of
pedigreed crosses).  All of our inferences will be sensitive to this
assumption, but we can check how sensitive (by trying different assumptions
or doing other mathematical sensitivity analysis).

The particular experiment at hand has five subexperiments laid out as follows.
\begin{center}
\begin{tabular}{ccc}
\toprule
year one & year two & year three \\
\midrule
parents & parents & parents \\
        & offspring & offspring \\
\bottomrule
\end{tabular}
\end{center}
Here ``parents'' means individuals produced by the experimental crosses
(seeds from
the same crosses were planted in each year).  For short, we will say these
individuals are from the same ``family'' if they are known sibs
(that is, have the same sire in the experimental crosses).
Here ``offspring'' means individuals whose dam was a ``parent'' from the
previous year (their own sire unknown because of open pollination).
For short, we will say these
individuals are from the same ``family'' as their dam in the previous year.

\subsection{Change in Mean Fitness} \label{sec:change}

If
\begin{itemize}
\item $p_i$ is the (assumed) population frequency of family $i$
    (perhaps assumed to be equal for all $i$, perhaps not), and
\item $\hat{\mu}_i$ is (estimated) mean fitness for family $i$
    (this comes from an aster model with random effects as shown by
    \citet{kulbaba} and \citet{zenodo},
\end{itemize}
then
\begin{itemize}
\item $\sum_i p_i \hat{\mu}_i$ is (estimated) mean fitness for
that subexperiment, where the sum runs over all families.
\end{itemize}

We also need to know the (estimated) population frequency
\emph{after selection}, which is
\begin{equation*}
   \hat{q}_i = \frac{p_i \hat{\mu}_i}{\sum_j p_j \hat{\mu}_j}
\end{equation*}
where the sum in the denominator runs over all families.
(This is standard in quantitative genetics --- \citet{lande-arnold},
breeder's equation,
and so forth --- but does not account for recombination and other effects
\citep{pr70,pr72}.)

Now we need to consider three subexperiments
\begin{itemize}
\item a ``parent'' subexperiment in one year (denoted by superscript prev),
\item an ``offspring'' subexperiment in the next year (denoted
    by superscript off), and
\item the ``parent'' subexperiment in the same year as the ``offspring
    experiment (denoted by superscript par).
\end{itemize}
(There are two such triangles of subexperiments in these data, one
for each offspring year.)

The total change in mean fitness in from one generation to (the same point in) the next is
\begin{equation} \label{eq:total-change}
   \sum\nolimits_i \bigl(
   \hat{q}_i^\text{prev} \hat{\mu}_i^\text{off}
   -
   p_i^\text{prev} \hat{\mu}_i^\text{prev}
   \bigr)
\end{equation}

\subsection{Decomposition of Change in Mean Fitness} \label{sec:decompose}

We now decompose \eqref{eq:total-change} into a sum of three terms,
each of which has a biological interpretation.

First we look at the change in mean fitness due to change in environment
(between the two years involved in this analysis), which we take to be
\begin{subequations}
\begin{equation} \label{eq:partial-change-environmental}
   \sum\nolimits_i p_i^\text{prev}
   \bigl( \hat{\mu}_i^\text{par} - \hat{\mu}_i^\text{prev} \bigr)
\end{equation}

Subtracting \eqref{eq:partial-change-environmental} from \eqref{eq:total-change}
gives
\begin{equation*}
   \sum\nolimits_i \bigl(
   \hat{q}_i^\text{prev} \hat{\mu}_i^\text{off}
   -
   p_i^\text{prev} \hat{\mu}_i^\text{par}
   \bigr)
\end{equation*}
which we take to be the change in fitness due to genetics (we are now
comparing parents and offspring in the same environment).

Fisher's fundamental theorem of natural selection (FFTNS) does not try
to predict all change in mean fitness but only that
``due to natural selection'' which is often taken to be
\begin{equation} \label{eq:partial-change-fftns}
   \sum\nolimits_i
   \bigl(\hat{q}_i^\text{prev} - p_i^\text{prev}\bigr) \hat{\mu}_i^\text{par}
\end{equation}

Subtracting \eqref{eq:partial-change-fftns} from the unnumbered equation
preceding it gives
\begin{equation} \label{eq:partial-change-genetic-non-fftns}
    \sum\nolimits_i \hat{q}_i^\text{prev} \bigl(\hat{\mu}_i^\text{off} -
    \hat{\mu}_i^\text{par}\bigr)
\end{equation}
which we take to be the part of change in mean fitness that is genetic
but not predicted by FFTNS (not ``due to natural selection'').
\end{subequations}

It is easily checked that
\eqref{eq:partial-change-environmental} plus
\eqref{eq:partial-change-fftns} plus
\eqref{eq:partial-change-genetic-non-fftns} gives
\eqref{eq:total-change}.  Thus we have decomposed the total change
in mean fitness in one generation into three parts, one of which is
the part of the change addressed by FFTNS and the
others have biological interpretations.

We intend to estimate, discuss, and interpret each of these three differences
separately.  We also need to derive standard errors for each.

\section{Theory}

\subsection{Aster Models with Random Effects}

In aster models with random effects \citep*{reaster} we have fixed effects
and random effects.  Fixed effects are unknown parameters to estimate.
Random effects are not.  They are (theoretical, hypothetical, unobservable,
latent, hidden, and perhaps other names) random variables whose variances are
unknown parameters to estimate (so-called \emph{variance components}).

R function \code{reaster} in R package \code{aster} \citep{aster-package}
provides standard errors for the estimated parameters (fixed effects and
variance components) and, more generally, provides (an approximation to)
the inverse Fisher information matrix for these parameters, which is the
asymptotic variance-covariance matrix of their sampling distribution.
Since version 1.2-1 of R package \code{aster} this variance-covariance
matrix is provided by a method of R generic
function \code{vcov} that handles objects of class \code{reaster} (which
are objects returned by R function \code{reaster}).

\subsection{Estimates of Random Effects}

R function \code{reaster} also provides ``estimates'' (in scare quotes)
of random effects.  Of course, random effects are not constants to estimate,
so these are not like parameter estimates.  They estimate \emph{something}, but
what?  (In Section~\ref{sec:m-estimation} below we change our tune and do
consider random effects as unknown parameters to estimate, thus just like
fixed effects except that we use regularization/shrinkage for random
effects but not for fixed effects.)

In classical quantitative genetics where all random variables in the model
(response and random effects) are jointly multivariate normal, one has
BLUP (best linear unbiased predictors) of the random effects.  These
are conditional expectations of random effects given the observed value
of the response (quantitative trait), and these are linear functions
of the response vector (because conditional expectations of multivariate
normal are linear in the conditioning variables) and unbiased
(also a property of multivariate normal)
and best in the sense of having smaller variance than any other linear
unbiased predictions.

One can think of R function \code{reaster}'s ``estimates'' (in scare quotes)
of random effects as being analogs of BLUP's except without the B, L, and U.
(Anything we use as a prediction is thereby a prediction, hence they are
trivially still P.)
Because the response in an aster model is not taken to be normally distributed,
the conditional distribution of random effects given observed response is
not normally distributed (and has no formula so could only be calculated
using Markov chain Monte Carlo (MCMC)).  Hence we lose the linearity (L),
unbiasedness (U), and best (B), which came from multivariate normality.
We also lose conditional expectation (no letter), which BLUP are,
but \code{reaster} ``estimates'' (in scare quotes) of random effects are not.
(If we wanted conditional expectations, we would need MCMC to obtain them.)

So after all of that explanation of what these ``estimates'' (in scare quotes)
are not, what are they?  For that we dive into the theory in
\citet[Section~2]{reaster}.  There we have the notations $\alpha$ for
the vector of fixed effects, $b$ for the vector of random effects,
and $\nu$ for the vector of variance components, and
$\hat{\alpha}$, $\hat{b}$, and $\hat{\nu}$ for the estimates thereof.

Minus the \emph{complete data log likelihood} (what the log likelihood would
be if the random effects were observed data) is
\begin{equation} \label{eq:logl-complete}
   - l(a + M \alpha + Z b) + \tfrac{1}{2} b^T D^{-1} b
   + \tfrac{1}{2} \log \det(D)
\end{equation}
(equation (4) in \citet{reaster}, minus because they want to minimize
rather than maximize), where
\begin{itemize}
\item $l$ is the log likelihood function for the saturated aster model
    unconditional canonical parameter vector $\varphi$,
    which is a function of the response vector $y$ even though $y$
    does not appear explicitly in the right-hand side
    of \eqref{eq:logl-complete},
\item $a$ is the offset vector,
\item $M$ is the model matrix for fixed effects,
\item $Z$ is the model matrix for random effects, and
\item $D$ is the variance-covariance matrix for random effects,
    which is assumed to be diagonal with diagonal components that
    are equal to components of the parameter vector $\nu$ (so,
    although $\nu$ does not appear explicitly in the right-hand side
    of \eqref{eq:logl-complete}, it is there implicitly
    because $D$ is a function of $\nu$).
\end{itemize}
The notation $D^{-1}$ assumes the matrix $D$ is invertible, which it is
if none of the variance components are zero (the case of zero variance
components is handled by R function \code{reaster} and is discussed
by \citet[Section~3]{reaster} but we ignore that here; if the genetic variance
were estimated to be zero, then the $\hat{b}$ would be zero for
all individuals, and we would have no estimated change in mean fitness).

Considered as a function of $b$ for fixed $\alpha$, $\nu$, and $y$,
\eqref{eq:logl-complete} is minus the log unnormalized probability density
function of the
conditional distribution of $b$ given $y$.  As argued
by \citet[Section~2]{reaster},
for fixed $\alpha$, $\nu$, and $y$, \eqref{eq:logl-complete} has a unique
minimizer (minimizing over $b$), which we denote $b^*(\alpha, \nu, y)$.
Since this maximizes the conditional probability density, this is the
\emph{mode} of the conditional distribution of $b$ given $y$.
When we plug in approximate maximum likelihood estimates for the unknown
parameters we obtain
\begin{equation} \label{eq:b-hat}
   \hat{b} = b^*(\hat{\alpha}, \hat{\nu}, y)
\end{equation}
which are the $b$ returned by R function \code{reaster} and which are thus
\emph{estimates} of a feature (the mode) of the conditional distribution
of random effects given
observed data.  (BLUP have the same issue.  The term BLUP only describes
what would be obtained if the true unknown parameter values were used in
the calculation.  If estimates are used instead, then sometimes the
initialism EBLUP, for empirical BLUP, is used to indicate that.)
So if we want an initialism, the random effects estimates $\hat{b}$ returned
by R function \code{reaster}, are ECMP (empirical conditional mode predictors),
and the same calculation with true unknown parameter values used instead
of estimates would be CMP (conditional mode predictors).
CMP are what ECMP are estimating.

In hindsight BLUP are also CMP, because normal distributions (and
conditional distributions of multivariate normal are again multivariate normal)
are symmetric and unimodal so the mean, median, and mode are the same.
Thus, when thought about the right way, CMP are generalizations of BLUP.

In aster models with random effects, the conditional distribution of random
effects given observed data is not multivariate normal nor even symmetric.
Hence CMP are not medians or means.  We can only call them conditional modes.

\subsection{Estimates of Random Effects Nonlinearly Mapped}

For estimates of population mean fitness we are not interested in the
estimates \eqref{eq:b-hat} because they are on the wrong scale.  Fitness
is expected lifetime number of offspring, which is measured on the
unconditional mean value parameter scale whereas \eqref{eq:b-hat} are
on the unconditional canonical parameter scale.  The mapping from one
scale to the other can be calculated by the computer so the random effects
estimates \eqref{eq:b-hat} can be mapped to the mean value scale.
But the mapping is nonlinear, and nonlinear mappings need not preserve
modes of distributions.

So we can no longer call these modes.
They are modes on the canonical parameter scale but not necessarily
on the mean value scale.
Perhaps we can call them mapped conditional mode predictors (MCMP).

As a possible alternative, we could try to find the mode of the conditional
distribution of random effects mapped to the mean value parameter scale.
But we do not try out this suggestion.

\subsection{Two Ways of Thinking}

There are two ways of thinking about these estimates (whether on the
canonical or mean value scale).

We can think of them as properties
of the conditional distribution, and hence functions of the data.
This is what the notation above suggests: $b^*(\alpha, \nu, y)$ when
thought of as a function of $y$ is the mode of the conditional distribution
of the vector $b$ of random effects given the vector $y$ of observed data.
And \eqref{eq:b-hat} is what we get when we plug in parameter estimates
for the true unknown parameters.  In this interpretation $y$ is an ordinary
vector variable (like in mathematics outside of probability theory).

Or we can think of them as just more things to estimate.  This does not
make a lot of sense of what these quantities actually are, but does reflect
the actual estimation process described by \citet{reaster}.
In this interpretation $y$ is a random vector having the true unknown
distribution of the data.

Here we use the latter interpretation.

\subsection{Estimation and Standard Errors}
\label{sec:m-estimation}

\subsubsection{Computing}
\label{sec:m-estimation-computing}

Since version 1.2-1 of R package \code{aster} \citep{aster-package}
the standard errors for this scheme, which treats random effects as
parameters, just like fixed effects except that they
are regularized (shrunken), is implemented by the method of R generic
function \code{vcov} that handles objects of class \code{reaster}
(which are the values returned by R function \code{reaster})
when the optional
argument \code{re.too = TRUE} is provided to R function \code{vcov}.

Thus one may skip the following section, which provides the theory of
these variance-covariance estimates, if one just wants calculations.
The following section, however, is the only place where that theory
is written down, hence the only justification of these variance-covariance
estimates.

\subsubsection{Theory}

Following \citet{reaster} in the text surrounding their equation (7),
the vector $(\hat{\alpha}, \hat{b}, \hat{\nu})$ is the joint minimizer
of the function $p$ given by their equation (7) where $\widehat{W}$ is
considered fixed and known at some points of the argument and variable
(to be estimated) in other parts.  This is an approximation
\citet{reaster} attribute to \citet{breslow-clayton}.
For the purposes of calculating standard errors we will consider
$\widehat{W}$ as fixed (as does R function \code{reaster} in R package
\code{aster} \citep{aster-package}).

The general theory of $M$-estimation \citep[Section~5.3]{vdv} says
that the asymptotic variance of these estimators has the ``sandwich''
form $U^{-1} V (U^{-1})^T$, where
\begin{subequations}
\begin{align}
   U & = E_{\alpha, b, \nu} \bigl\{ \nabla^2 p(\alpha, b, \nu)) \bigr\}
   \label{eq:u}
   \\
   V & = \var_{\alpha, b, \nu} \bigl\{ \nabla p(\alpha, b, \nu)) \bigr\}
   \label{eq:v}
\end{align}
\end{subequations}
In these equations $\nabla^2$ denotes the square matrix of second partial
derivatives with respect to all three vectors. These second derivatives
are given by
\begin{align*}
   p_{\alpha \alpha}(\alpha, b, \nu)
   & =
   M^T W(a + M \alpha + Z b) M
   \\
   p_{\alpha b}(\alpha, b, \nu)
   & =
   M^T W(a + M \alpha + Z b) Z
   \\
   p_{\alpha \nu_k}(\alpha, b, \nu)
   & =
   0
   \\
   p_{b b}(\alpha, b, \nu)
   & =
   Z^T W(a + M \alpha + Z b) Z + D^{- 1}
   \\
   p_{b \nu_k}(\alpha, b, \nu)
   & =
   - D^{- 1} E_k D^{- 1} b
   \\
   p_{\nu_j \nu_k}(\alpha, b, \nu)
   & =
   b^T D^{- 1} E_j D^{- 1} E_k D^{- 1} b
   \\
   & \qquad
   -
   \tfrac{1}{2} \tr \Bigl(
   \bigl[ Z^T \widehat{W} Z D + I \bigr]^{- 1}
   Z^T \widehat{W} Z E_j
   \\
   & \qquad \qquad
   \bigl[ Z^T \widehat{W} Z D + I \bigr]^{- 1}
   Z^T \widehat{W} Z E_k
   \Bigr)
\end{align*}
\citep[unnumbered displayed equation at the end of Section~2]{reaster}.
These first derivatives are not given in the paper, but are given in
the backing technical report \citep[Section~1.7]{tr692}.
\begin{align*}
   p_\alpha(\alpha, b, \nu)
   & =
   - M^T \bigl[ y - \mu(a + M \alpha + Z b) \bigr]
   \\
   p_b(\alpha, b, \nu)
   & =
   - Z^T \bigl[ y - \mu(a + M \alpha + Z b) \bigr] + D^{- 1} b
   \\
   p_{\nu_k}(\alpha, b, \nu)
   & =
   - \tfrac{1}{2} b^T D^{- 1} E_k D^{- 1} b
   + \tfrac{1}{2} \tr \Bigl(
   \bigl[ Z^T \widehat{W} Z D + I \bigr]^{- 1}
   Z^T \widehat{W} Z E_k
   \Bigr)
\end{align*}

In these equations we are thinking of $b$ as a parameter we estimate
rather than as a random effect.  The motivation for the choice of the
function $p$ defined by equation (7) in \citet{reaster} is the random
effects story, but the actual estimation procedure is an $M$-estimation.
Thus we treat only $y$ as random in the equations in this section.

Since the second derivatives do not involve $y$, we consider them nonrandom,
hence their own expectations.  Thus we use the second derivatives as is,
except for putting hats on things.
\begin{align*}
   \widehat{U}_{\alpha \alpha}
   & =
   M^T \widehat{W} M
   \\
   \widehat{U}_{\alpha b}
   & =
   M^T \widehat{W} Z
   \\
   \widehat{U}_{\alpha \nu_k}
   & =
   0
   \\
   \widehat{U}_{b b}
   & =
   Z^T \widehat{W} Z + \widehat{D}^{- 1}
   \\
   \widehat{U}_{b \nu_k}
   & =
   - \widehat{D}^{- 1} E_k \widehat{D}^{- 1} \hat{b}
   \\
   \widehat{U}_{\nu_j \nu_k}
   & =
   \hat{b}^T \widehat{D}^{- 1} E_j \widehat{D}^{- 1} E_k \widehat{D}^{- 1} \hat{b}
   \\
   & \qquad
   -
   \tfrac{1}{2} \tr \Bigl(
   \bigl[ Z^T \widehat{W} Z \widehat{D} + I \bigr]^{- 1}
   Z^T \widehat{W} Z E_j
   % \\
   % & \qquad \qquad
   \bigl[ Z^T \widehat{W} Z \widehat{D} + I \bigr]^{- 1}
   Z^T \widehat{W} Z E_k
   \Bigr)
\end{align*}
Since the first derivatives are linear in $y$, their variances and covariances
are linear in $\var(y)$
\begin{align*}
   \widehat{V}_{\alpha \alpha}
   & =
   M^T \var_{\hat{\alpha}, \hat{b}, \hat{\nu}}(y) M
   \\
   \widehat{V}_{\alpha b}
   & =
   M^T \var_{\hat{\alpha}, \hat{b}, \hat{\nu}}(y) Z
   \\
   \widehat{V}_{\alpha \nu_k}
   & =
   0
   \\
   \widehat{V}_{b b}
   & =
   Z^T \var_{\hat{\alpha}, \hat{b}, \hat{\nu}}(y) Z
   \\
   \widehat{V}_{b \nu_k}
   & =
   0
   \\
   \widehat{V}_{\nu_j \nu_k}
   & =
   0
\end{align*}

We end this section where we began, with an apology for treating
$\widehat{W}$ as a known constant matrix in some parts of the argument,
in particular in deriving our equations for $\widehat{U}$ and $\widehat{V}$,
and as a function of $\alpha$ and $b$ in other parts of the argument
(and in the actual estimation done by R function \code{reaster}).

We have $\widehat{U}_{\alpha, \nu} = \widehat{V}_{\alpha, \nu} = 0$
because no term in equation (7) of \citet{reaster} contains both $\alpha$
and $\nu$ so mixed partial derivatives are zero.

But the argument why $\widehat{V}_{b \nu}$ and $\widehat{V}_{\nu \nu}$
are zero matrices involves two issues.
\begin{itemize}
\item In this section of this document, we are treating only $y$ (not $b$)
    as random, which is unlike what \citet{reaster} do.  They only use
    $\hat{b}$ in the process of deriving the approximated integrated
    likelihood from which $b$ has been eliminated.  Consequently, they
    do not consider $\hat{b}$ an estimator of anything so it has no
    standard errors.  Here we want to consider $\hat{b}$ an estimator
    (of something) and we need standard errors for it.  Hence the
    procedure here.
\item Because we are treating on $y$ as random, this makes
   $p_{\nu_k}(\alpha, b, \nu)$, which does not contain $y$, nonrandom.
   Hence it has variance zero, and covariance with any other random vector
   zero.
\end{itemize}

Thus the procedure proposed here, while different from that proposed
by \citet{reaster} does make sense in the current context.

The approximate asymptotic variance-covariance matrix for the vector
$(\hat{\alpha}, \hat{b}, \hat{\nu})$ discussed in this section is
implemented in R function \code{vcov.reaster} in
the current version (not yet on CRAN) of R package \code{aster}
\citep{aster-package}.

\subsection{The Delta Method}

As explained in Section~\ref{sec:m-estimation-computing} above,
R generic function \code{vcov} with optional arguments
\code{re.too = TRUE} and \code{standard.deviation = FALSE} can
be used to obtain the asymptotic variance covariance for what R
function \code{reaster} considers the parameters, the vector \code{alpha}
of fixed effects, the vector \code{b} of random effects, and the vector
\code{nu} of variance components.

To put these parameter estimates through further transformations, we
apply the multivariate delta method.  Call the whole parameter vector
$\theta$ (the concatenation of $\alpha$, $b$, and $\nu$).  If $\theta_0$
is the true unknown parameter value and $g$ is a vector-to-vector
function that is differentiable at $\theta_0$ with derivative matrix
(also called Jacobian matrix) $g'(\theta_0)$, then the multivariate delta
method says: if $\hat{\theta}$ is approximately multivariate normal
with mean vector $\theta$ and variance-covariance matrix $\Sigma$,
then $g(\hat{\theta})$ is approximately multivariate normal
with mean vector $g(\theta)$ and variance-covariance matrix
$g'(\theta_0)^T \Sigma g'(\theta_0)$.

We follow this literally.
\begin{itemize}
\item Code the mathematical function $g$ as an R function
\item Use R function \code{jacobian} in R package \code{numDeriv} to
    calculate the Jacobian matrix $g'(\hat{\theta})$ which estimates
    $g'(\theta_0)$.
\item Use R function \code{vcov} in R package \code{aster} with optional
    arguments \code{re.too = TRUE} and \code{standard.deviation = FALSE}
    to compute the approximate variance-covariance matrix of $\hat{\theta}$.
    Call that $\widehat{\Sigma}$.
\item Compute $g'(\hat{\theta})^T \widehat{\Sigma} g'(\hat{\theta})$.
\end{itemize}

When we do another stage of transformation, we do the same thing, except
now we are calling the vector of estimates we are starting this step with
$\hat{\theta}$ (what was $g(\hat{\theta})$ before) and we are calling its
true unknown parameter value $\theta_0$ and its
approximate variance-covariance matrix $\widehat{\Sigma}$ (what we computed
in our previous application or applications of the delta method) and now
$g$ denotes some new parameter transformation function.  Then the delta
method says:
\begin{itemize}
\item Code the mathematical function $g$ as an R function
\item Use R function \code{jacobian} in R package \code{numDeriv} to
    calculate the Jacobian matrix $g'(\hat{\theta})$ which estimates
    $g'(\theta_0)$.
\item Compute $g'(\hat{\theta})^T \widehat{\Sigma} g'(\hat{\theta})$.
\end{itemize}
(so $\widehat{\Sigma}$ is the variance-covariance matrix computed in the
previous stage, rather than the output of R function \code{vcov}).

\begin{thebibliography}{}

\bibitem[Breslow and Clayton(1993)]{breslow-clayton}
Breslow, N.~E., and Clayton, D.~G. (1993).
\newblock Approximate inference in generalized linear mixed models.
\newblock \emph{Journal of the American Statistical Association},
    \textbf{88}, 9--25.
\newblock \doi{10.1080/01621459.1993.10594284}.

\bibitem[Geyer(2024)]{aster-package}
Geyer, C.~J. (2024).
\newblock R package \texttt{aster}: Aster Models, version 1.2-1.
\newblock \url{https://cran.r-project.org/package=aster}.

\bibitem[Geyer(unpublished)]{aster-theory}
Geyer, C.~J. (unpublished).
\newblock \emph{Aster Theory}.
\newblock \url{https://github.com/cjgeyer/AsterTheory}.
\newblock There are no version numbers, so cite the specific git
    commit by its SHA-1 checksum.

\bibitem[Geyer, et al.(2022)Geyer, Kulbaba, Sheth, Pain, Eckhart,
    and Shaw]{zenodo}
Geyer, C.~J., Kulbaba, M.~W., Sheth, S.~N., Pain, R.~E., Eckhart, V.~M.,
    and Shaw, R.~G. (2022).
\newblock Correction for Kulbaba et al. (2019).
\newblock \emph{Evolution}, \textbf{76}, 3074.
\newblock \doi{10.1111/evo.14607}.
\newblock Supplementary material, version 2.0.1.
\newblock \doi{10.5281/zenodo.7013098}.

\bibitem[Geyer, et al.(2012)Geyer, Ridley, Latta, Etterson, and Shaw]{tr692}
Geyer, C.~J., Ridley, C.~E., Latta, R.~G., Etterson, J.~R and Shaw, R.~G.
    (2012).
\newblock Aster Models with Random Effects via Penalized Likelihood.
\newblock Technical Report No. 692.  School of Statistics,
    University of Minnesota.
\newblock \url{https://hdl.handle.net/11299/135870}.

\bibitem[Geyer, et al.(2013)Geyer, Ridley, Latta, Etterson, and Shaw]{reaster}
Geyer, C.~J., Ridley, C.~E., Latta, R.~G., Etterson, J.~R., and Shaw, R.~G.
    (2013).
\newblock Local Adaptation and genetic effects on fitness: Calculations for
    exponential family models with random effects.
\newblock \emph{Annals of Applied Statistics}, \textbf{7}, 1778--1795.
\newblock \doi{10.1214/13-AOAS653}.

\bibitem[Kulbaba, et al.(2019)Kulbaba, Sheth, Pain, Eckhart, and Shaw]{kulbaba}
Kulbaba, M.~W., Sheth, S.~N., Pain, R.~E., Eckhart, V.~M., and
    Shaw, R.~G. (2019).
\newblock Additive genetic variance for lifetime fitness and the capacity
    for adaptation in an annual plant.
\newblock \emph{Evolution}, \textbf{73}, 1746--1758.
\newblock \doi{10.1111/evo.13830}.
\newblock Correction: \citet{zenodo}.

\bibitem[Lande and Arnold(1983)]{lande-arnold}
Lande, R. and Arnold, S.~J. (1983).
\newblock The measurement of selection on correlated characters.
\newblock \emph{Evolution}, \textbf{37}, 1210--1226.

\bibitem[Price(1970)]{pr70}
Price, G.~R. (1970).
\newblock Selection and covariance.
\newblock \emph{Nature}, \textbf{227}, 520--521.

\bibitem[Price(1972)]{pr72}
Price, G.~R. (1972).
\newblock Extension of covariance selection mathematics.
\newblock \emph{Annals of Human Genetics}, \textbf{35}, 485--490.

\bibitem[van der Vaart(1998)]{vdv}
van der Vaart, A.~W. (1998).
\newblock \emph{Asymptotic Statistics}.
\newblock Cambridge University Press.

\end{thebibliography}

\end{document}
