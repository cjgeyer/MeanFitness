
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{natbib}
\usepackage[colorlinks=true,allcolors=blue]{hyperref}
\usepackage{doi}

\newcommand{\fatdot}{\,\cdot\,}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\tr}{tr}

\let\code=\texttt

\newcommand{\REVISED}{\begin{center} \LARGE REVISED DOWN TO HERE \end{center}}
\newcommand{\MOVED}[1][equation]{\begin{center} [#1 moved] \end{center}}

\begin{document}

\title{Decomposing Change in Population Mean Fitness}

\author{Charles J. Geyer \and Ruth G. Shaw}

\maketitle

\begin{abstract}
We previously measured mean fitness and additive genetic variance
under wild conditions in an annual plant \emph{Chamaecrista fasciculata}.
Here we analyze change in mean fitness across one generation and
decompose that into three parts: change predicted by Fisher's
fundamental theorem of natural selection, environmental change,
and the part of genetic change not addressed by Fisher's fundamental
theorem.  We also provide standard errors for these estimates.
\end{abstract}

\section{Estimates}

\subsection{Experiment}

We have an experiment \citep{kulbaba,zenodo} in which (Darwinian) fitness
(measured as lifetime
number of offspring per individual) was measured under wild conditions.
The individuals in the experiment were not a random sample from
a population but rather offspring from crosses in the experimental design having distinct sets of maternal
individuals mated to individual paternal ones, that is,
generating full-sib groups nested within paternal half-sib
groups.  There are no data on
population allele frequencies, and even if there were, there would be
no information of the bearing of the alleles on fitness of individuals in the population. 
Thus we simply assume some population distribution (of
pedigreed crosses).  All of our inferences will be sensitive to this
assumption, but we can check how sensitive (by trying different assumptions
or doing other mathematical sensitivity analysis).

The particular experiment at hand has five subexperiments laid out as follows.
\begin{center}
\begin{tabular}{ccc}
\toprule
year one & year two & year three \\
\midrule
parents & parents & parents \\
        & offspring & offspring \\
\bottomrule
\end{tabular}
\end{center}
Here ``parents'' means individuals produced by the experimental crosses
(seeds from
the same crosses were planted in each year).  For short, we will say these
individuals are from the same ``family'' if they are known sibs
(that is, have the same sire in the experimental crosses).
Here ``offspring'' means individuals whose dam was a ``parent'' from the
previous year (their own sire unknown because of open pollination).
For short, we will say these
individuals are from the same ``family'' as their dam in the previous year.

\subsection{Change in Mean Fitness}

If
\begin{itemize}
\item $p_i$ is the (assumed) population frequency of family $i$
    (perhaps assumed to be equal for all $i$, perhaps not), and
\item $\hat{\mu}_i$ is (estimated) mean fitness for family $i$
    (this comes from an aster model with random effects as shown by
    \citet{kulbaba} and \citet{zenodo},
\end{itemize}
then
\begin{itemize}
\item $\sum_i p_i \hat{\mu}_i$ is (estimated) mean fitness for
that subexperiment, where the sum runs over all families.
\end{itemize}

We also need to know the (estimated) population frequency
\emph{after selection}, which is
\begin{equation*}
   \hat{q}_i = \frac{p_i \hat{\mu}_i}{\sum_j p_j \hat{\mu}_j}
\end{equation*}
where the sum in the denominator runs over all families.
(This is standard in quantitative genetics, \citet{lande-arnold},
breeder's equation,
and so forth, but does not account for recombination and other effects
\citep{pr70,pr72}.)

Now we need to consider three subexperiments
\begin{itemize}
\item a ``parent'' subexperiment in one year (denoted by superscript prev),
\item an ``offspring'' subexperiment in the next year (denoted
    by superscript off), and
\item the ``parent'' subexperiment in the same year as the ``offspring
    experiment (denoted by superscript par).
\end{itemize}
(There are two such triangles of subexperiments in these data, one
for each offspring year.)

The total change in mean fitness in from one generation to (the same point in) the next is
\begin{equation} \label{eq:total-change}
   \sum\nolimits_i \bigl(
   \hat{q}_i^\text{prev} \hat{\mu}_i^\text{off}
   -
   p_i^\text{prev} \hat{\mu}_i^\text{prev}
   \bigr)
\end{equation}

\subsection{Decomposition of Change in Mean Fitness}

We now decompose \eqref{eq:total-change} into a sum of three terms,
each of which has a biological interpretation.

First we look at the change in mean fitness due to change in environment
(between the two years involved in this analysis), which we take to be
\begin{subequations}
\begin{equation} \label{eq:partial-change-environmental}
   \sum\nolimits_i p_i^\text{prev}
   \bigl( \hat{\mu}_i^\text{par} - \hat{\mu}_i^\text{prev} \bigr)
\end{equation}

Subtracting \eqref{eq:partial-change-environmental} from \eqref{eq:total-change}
gives
\begin{equation*}
   \sum\nolimits_i \bigl(
   \hat{q}_i^\text{prev} \hat{\mu}_i^\text{off}
   -
   p_i^\text{prev} \hat{\mu}_i^\text{par}
   \bigr)
\end{equation*}
which we take to be the change in fitness due to genetics (we are now
comparing parents and offspring in the same environment).

Fisher's fundamental theorem of natural selection (FFTNS) does not try
to predict all change in mean fitness but only that
``due to natural selection'' which is often taken to be
\begin{equation} \label{eq:partial-change-fftns}
   \sum\nolimits_i
   \bigl(\hat{q}_i^\text{prev} - p_i^\text{prev}\bigr) \hat{\mu}_i^\text{par}
\end{equation}

Subtracting \eqref{eq:partial-change-fftns} from the unnumbered equation
preceding it gives
\begin{equation} \label{eq:partial-change-genetic-non-fftns}
    \sum\nolimits_i \hat{q}_i^\text{prev} \bigl(\hat{\mu}_i^\text{off} -
    \hat{\mu}_i^\text{par}\bigr)
\end{equation}
which we take to be the part of change in mean fitness that is genetic
but not predicted by FFTNS (not ``due to natural selection'').
\end{subequations}

It is easily checked that
\eqref{eq:partial-change-environmental} plus
\eqref{eq:partial-change-fftns} plus
\eqref{eq:partial-change-genetic-non-fftns} gives
\eqref{eq:total-change}.  Thus we have decomposed the total change
in mean fitness in one generation into three parts, one of which is
the part of the change addressed by FFTNS and the
others have biological interpretations.

We intend to estimate, discuss, and interpret each of these three differences
separately.  We also need to derive standard errors for each.

\section{Standard Errors}

\subsection{Introduction}

In aster models with random effects \citep*{reaster} we have fixed effects
and random effects.  Fixed effects are unknown parameters to estimate.
Random effects are not.  They are (theoretical, hypothetical, unobservable,
latent, hidden, and perhaps other names) random variables whose variances are
unknown parameters to estimate (so-called \emph{variance components}).

R function \code{reaster} in R package \code{aster} \citep{aster-package}
provides standard errors for the estimated parameters (fixed effects and
variance components) and, more generally, provides (an approximation to)
the Fisher information matrix for these parameters.  The latter is
undocumented although is used in published examples
\citep[Supplementary material Section~12.3.1]{zenodo}.  And the inverse
of this matrix is (by the ``usual'' asymptotics of maximum likelihood)
the variance-covariance matrix of the parameter estimates.

R function \code{reaster} also provides ``estimates'' (in scare quotes)
of random effects.  Of course, random effects are not constants to estimate,
so these are not like parameter estimates.  They estimate \emph{something}, but
what?

In classical quantitative genetics where all random variables in the model
(response and random effects) are jointly multivariate normal, one has
BLUP (best linear unbiased predictors) of the random effects.  These
are conditional expectations of random effects given the observed value
of the response (quantitative trait), and these are linear functions
of the response vector (because conditional expectations of multivariate
normal are linear in the conditioning variables) and unbiased
(also a property of multivariate normal)
and best in the sense of having smaller variance than any other linear
unbiased predictions.

One can think of R function \code{reaster}'s ``estimates'' (in scare quotes)
of random effects as being analogs of BLUP's except without the B, L, and U.
(Anything we use as a prediction is thereby a prediction, hence they are
trivially still P.)
Because the response in an aster model is not taken to be normally distributed,
the conditional distribution of random effects given observed response is
not normally distributed (and has no formula so could only be calculated
using Markov chain Monte Carlo (MCMC)).  Hence we lose the linearity (L),
unbiasedness (U), and best (B), which came from multivariate normality.
We also lose conditional expectation (no letter), which BLUP are,
but \code{reaster} ``estimates'' (in scare quotes) of random effects are not.
(If we wanted conditional expectations, we would need MCMC to obtain them.)

So after all of that explanation of what these ``estimates'' (in scare quotes)
are not, what are they?  For that we dive into the theory in
\citet[Section~2]{reaster}.  There we have the notations $\alpha$ for
the vector of fixed effects, $b$ for the vector of random effects,
and $\nu$ for the vector of variance components, and
$\hat{\alpha}$, $\hat{b}$, and $\hat{\nu}$ for the estimates thereof.

Minus the \emph{complete data log likelihood} (what the log likelihood would
be if the random effects were observed data) is
\begin{equation} \label{eq:logl-complete}
   - l(a + M \alpha + Z b) + \tfrac{1}{2} b^T D^{-1} b
   + \tfrac{1}{2} \log \det(D)
\end{equation}
(equation (4) in \citet{reaster}, minus because they want to minimize
rather than maximize), where
\begin{itemize}
\item $l$ is the log likelihood function for the saturated aster model
    unconditional canonical parameter vector $\varphi$,
    which is a function of the response vector $y$ even though $y$
    does not appear explicitly in the right-hand side
    of \eqref{eq:logl-complete},
\item $a$ is the offset vector,
\item $M$ is the model matrix for fixed effects,
\item $Z$ is the model matrix for random effects, and
\item $D$ is the variance-covariance matrix for random effects,
    which is assumed to be diagonal with diagonal components that
    are equal to components of the parameter vector $\nu$ (so,
    although $\nu$ does not appear explicitly in the right-hand side
    of \eqref{eq:logl-complete}, it is there implicitly
    because $D$ is a function of $\nu$).
\end{itemize}
The notation $D^{-1}$ assumes the matrix $D$ is invertible, which it is
if none of the variance components are zero (the case of zero variance
components is handled by R function \code{reaster} and is discussed
by \citet{reaster} but we ignore that here; if the genetic variance
were estimated to be zero, then the $\hat{b}$ would be zero for
all individuals, and we would have no estimated change in mean fitness).

Considered as a function of $b$ for fixed $\alpha$, $\nu$, and $y$,
\eqref{eq:logl-complete} is minus the log unnormalized probability density
function of the
conditional distribution of $b$ given $y$.  As argued by \citet{reaster},
for fixed $\alpha$, $\nu$, and $y$, \eqref{eq:logl-complete} has a unique
minimizer (minimizing over $b$), which we denote $b^*(\alpha, \nu, y)$.
Since this maximizes the conditional probability density, this is the
\emph{mode} of the conditional distribution of $b$ given $y$.
When we plug in approximate maximum likelihood estimates for the unknown
parameters we obtain
\begin{equation*}
   \hat{b} = b^*(\hat{\alpha}, \hat{\nu}, y)
\end{equation*}
which are the $b$ returned by R function \code{reaster} and which are thus
\emph{estimates} of the conditional distribution of random effects given
observed data.  (BLUP have the same issue.  The term BLUP only describes
what would be obtained if the true unknown parameter values were used in
the calculation.  If estimates are used instead, then sometimes the
initialism EBLUP, for empirical BLUP, is used to indicate that.)
So if we want an initialism, the random effects estimates $\hat{b}$ returned
by R function \code{reaster}, are ECMP (empirical conditional mode predictors),
and the same calculation with true unknown parameter values used instead
of estimates would be CMP (conditional mode predictors).
CMP are what ECMP are estimating.

In hindsight BLUP are also CMP, because normal distributions (and
conditional distributions of multivariate normal are again multivariate normal)
are symmetric and unimodal so the mean, median, and mode are the same.
Thus, when thought about the right way, CMP are generalizations of BLUP.

One tricky thing about CMP: we are not thinking of $y$ as a random variable
but rather as an ordinary variable (as in mathematics outside of probability
theory).  A conditional mode (or a conditional \emph{anything}) is a function
of the conditioning variable.
So we can think of $b^*(\hat{\alpha}, \hat{\nu}, \fatdot)$ as a function
(of its third argument).  R function \code{reaster} returns the value
of this function for the observed data, but we could, in principle, evaluate
this function for any data.  So the error in our ECMP is not in $y$, it is
in $\hat{\alpha}$ and $\hat{\nu}$ not being equal to (the true unknown)
$\alpha$ and $\nu$.

Standard errors for any function of parameters are given by the multivariable
delta method.  To apply the delta method to $b^*$ we need derivatives
with respect to its first two arguments.

These derivatives are given in the technical report \citep{tr692} that
is the supplementary material for the paper \citet{reaster}.  Equations
(16) and (17) in that technical report are
\begin{subequations}
\begin{align}
   b^*_\alpha(\alpha, \nu)
   & =
   -
   p_{b b}(\alpha, b^*, \nu)^{-1} p_{b \alpha}(\alpha, b^*, \nu)
   \label{eq:b-star-alpha}
   \\
   b^*_{\nu}(\alpha, \nu)
   & =
   -
   p_{b b}(\alpha, b^*, \nu)^{-1} p_{b \nu}(\alpha, b^*, \nu)
   \label{eq:b-star-nu}
\end{align}
\end{subequations}
where \citet{tr692} drop the dependence of $b^*$ on $y$ from their
notation and $p$
can be taken to be
\begin{equation} \label{eq:pee}
   p(\alpha, b, \nu) =
   - l(a + M \alpha + Z b) + \tfrac{1}{2} b^T D^{-1} b
\end{equation}
(the terms of \eqref{eq:logl-complete} that depend on $b$; this is not
what \citet{tr692} take $p$ to be, but their definition agrees with ours
in the terms that depend on $b$). 
In \eqref{eq:b-star-alpha} and \eqref{eq:b-star-nu} subscripts
denote partial derivatives: $b^*_\alpha(\alpha, \nu)$ is a vector
whose components are $\partial b^*(\alpha, \nu) / \partial \alpha_i$
and similarly for derivatives with respect to components of $\nu$.

The second derivatives that appear on the right-hand side of
\eqref{eq:b-star-alpha} and \eqref{eq:b-star-nu} are given
in Section~1.8 of \citet{tr692} and repeated here
\begin{equation} \label{eq:second-pee}
\begin{split}
   p_{b \alpha}(\alpha, b, \nu)
   & =
   M^T W(a + M \alpha + Z b) Z
   \\
   p_{b b}(\alpha, b, \nu)
   & =
   Z^T W(a + M \alpha + Z b) Z + D^{- 1}
   \\
   p_{b \nu_k}(\alpha, b, \nu)
   & =
   - D^{- 1} E_k D^{- 1} b
\end{split}
\end{equation}
where
\begin{equation*}
   E_k = D_{\nu_k}(\nu)
\end{equation*}
is the diagonal matrix whose components are equal to one
if the corresponding components of $D$ are equal to $\nu_k$ by
definition (rather than by accident when some other component of $\nu$
also has the same value) and whose components are otherwise zero
and $p_{b \alpha}(\alpha, b, \nu)$ is the matrix whose $i, j$ component is
$\partial^2 p_{b \alpha}(\alpha, b, \nu) / \partial b_i \partial \alpha_j$.

So now we need to know whether there is a function in R package \code{aster}
that calculates these derivatives
\eqref{eq:b-star-alpha} and \eqref{eq:b-star-nu}.  Unfortunately,
the answer to that seems to be no.
R function \code{quickle} does calculate the second derivatives
\eqref{eq:second-pee} but uses them to calculate something else
(so it does not return either \eqref{eq:second-pee} or
\eqref{eq:b-star-alpha} and \eqref{eq:b-star-nu}, hence we can
copy some of the code of that function in our own function to calculate
\eqref{eq:b-star-alpha} and \eqref{eq:b-star-nu}).

\begin{thebibliography}{}

% \bibitem[Breslow and Clayton(1993)]{breslow-clayton}
% Breslow, N.~E., and Clayton, D.~G. (1993).
% \newblock Approximate inference in generalized linear mixed models.
% \newblock \emph{Journal of the American Statistical Association},
%     \textbf{88}, 9--25.
% \newblock \doi{10.1080/01621459.1993.10594284}.

\bibitem[Geyer(2021)]{aster-package}
Geyer, C.~J. (2021).
\newblock R package \texttt{aster}: Aster Models, version 1.1-2.
\newblock \url{https://cran.r-project.org/package=aster}.

\bibitem[Geyer, et al.(2022)Geyer, Kulbaba, Sheth, Pain, Eckhart,
    and Shaw]{zenodo}
Geyer, C.~J., Kulbaba, M.~W., Sheth, S.~N., Pain, R.~E., Eckhart, V.~M.,
    and Shaw, R.~G. (2022).
\newblock Correction for Kulbaba et al. (2019).
\newblock \emph{Evolution}, \textbf{76}, 3074.
\newblock \doi{10.1111/evo.14607}.
\newblock Supplementary material, version 2.0.1.
\newblock \doi{10.5281/zenodo.7013098}.

\bibitem[Geyer, et al.(2012)Geyer, Ridley, Latta, Etterson, and Shaw]{tr692}
Geyer, C.~J., Ridley, C.~E., Latta, R.~G., Etterson, J.~R and Shaw, R.~G.
    (2012).
\newblock Aster Models with Random Effects via Penalized Likelihood.
\newblock Technical Report No. 692.  School of Statistics,
    University of Minnesota.
\newblock \url{https://hdl.handle.net/11299/135870}.

\bibitem[Geyer, et al.(2013)Geyer, Ridley, Latta, Etterson, and Shaw]{reaster}
Geyer, C.~J., Ridley, C.~E., Latta, R.~G., Etterson, J.~R., and Shaw, R.~G.
    (2013).
\newblock Local Adaptation and genetic effects on fitness: Calculations for
    exponential family models with random effects.
\newblock \emph{Annals of Applied Statistics}, \textbf{7}, 1778--1795.
\newblock \doi{10.1214/13-AOAS653}.

\bibitem[Kulbaba, et al.(2019)Kulbaba, Sheth, Pain, Eckhart, and Shaw]{kulbaba}
Kulbaba, M.~W., Sheth, S.~N., Pain, R.~E., Eckhart, V.~M., and
    Shaw, R.~G. (2019).
\newblock Additive genetic variance for lifetime fitness and the capacity
    for adaptation in an annual plant.
\newblock \emph{Evolution}, \textbf{73}, 1746--1758.
\newblock \doi{10.1111/evo.13830}.
\newblock Correction: \citet{zenodo}.

\bibitem[Lande and Arnold(1983)]{lande-arnold}
Lande, R. and Arnold, S.~J. (1983).
\newblock The measurement of selection on correlated characters.
\newblock \emph{Evolution}, \textbf{37}, 1210--1226.

\bibitem[Price(1970)]{pr70}
Price, G.~R. (1970).
\newblock Selection and covariance.
\newblock \emph{Nature}, \textbf{227}, 520--521.

\bibitem[Price(1972)]{pr72}
Price, G.~R. (1972).
\newblock Extension of covariance selection mathematics.
\newblock \emph{Annals of Human Genetics}, \textbf{35}, 485--490.

% \bibitem[van der Vaart(1998)]{vdv}
% van der Vaart, A.~W. (1998).
% \newblock \emph{Asymptotic Statistics}.
% \newblock Cambridge University Press.

\end{thebibliography}

\end{document}
