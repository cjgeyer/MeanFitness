---
title: "Estimates of Realized Response to Selection in *Chamaecrista fasciculata* and Decomposition into Environmental and Genetic Parts"
author:
  - "Charles J. Geyer^[School of Statistics, University of Minnesota, geyer@umn.edu, https://orcid.org/0000-0003-1471-1703]"
  - "Mason W. Kulbaba^[St. Mary's University, mason.kulbaba@stmu.ca, https://orcid.org/0000-0003-0619-7089]"
  - "Seema N. Sheth^[Department of Plant and Microbial Biology, North Carolina State University, ssheth3@ncsu.edu, https://orcid.org/0000-0001-8284-7608]"
  - "Rachel E. Pain^[Ecology, Evolution and Behavior Graduate Program, University of Minnesota, repain@umn.edu]"
  - "Vincent M. Eckhart^[Department of Biology, Grinnell College, eckhart@grinnell.edu]"
  - "Ruth G. Shaw^[Department of Ecology, Evolution and Behavior, University of Minnesota, shawx016@umn.edu, https://orcid.org/0000-0001-5980-9291]"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  bookdown::pdf_document2:
    extra_dependencies: "amscd"
    number_sections: true
    toc: true
    toc_depth: 3
linkcolor: blue
urlcolor: blue
bibliography: foo.bib
csl: journal-of-the-royal-statistical-society.csl
link-citations: true
---

# Abstract {-}

This work builds on @kulbaba-et-al and the correction to it [@zenodo]
to obtain estimates of the realized response to natural selection.
Those articles
presented estimates of mean fitness and additive genetic variance for fitness
for three populations of *Chamaecrista fasciculata*,
each grown in its home location in three years via aster analyses
of records of components of fitness for a pedigreed set of individuals.
Here, we consider the realized change in mean fitness from one generation
to the next, for comparison with the prediction from Fisher's
Fundamental Theorem of Natural Selection (FFTNS).
We divide change in mean fitness in one generation
into three parts:
that due to change in genetic composition described by FFTNS
(intragenerational change in mean additive genetic effects for fitness),
that due to change in genetic composition not described by FFTNS (everything
else at least partially genetic), and that due to change in environment.
Here, we obtain estimates of a) mean fitness of the pedigreed parental
populations before selection (previously presented in @kulbaba-et-al and its
correction); b) mean fitness of the pedigreed parental population after
selection (i.e. accounting for the change in representation of the families
reflected in differential seed production); and mean fitness of the offspring
of the pedigreed sets
(i. e., the outcome of natural selection on the parental
generation when grown in the same sites in the following year).

We also obtain standard errors of our estimates.
In this we use a new scheme that treats random effects as parameters
to estimate because we do use estimates of random effects in our estimates
of mean fitness.

# License

This work is licensed under a Creative Commons
CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
(https://creativecommons.org/publicdomain/zero/1.0/).

The R markdown source for this document is the file `realized.Rmd`
in the GitHub private repository https://github.com/cjgeyer/mf
which will be made public whenever a paper based on it is submitted.

# R

 * The version of R used to make this document is `r getRversion()`.

 * The version of the `rmarkdown` package used to make this document is
   `r packageVersion("rmarkdown")`.

 * The version of the `bookdown` package used to make this document is
   `r packageVersion("bookdown")`.

 * The version of the `aster` package used to make this document is
   `r packageVersion("aster")`.

 * The version of the `numDeriv` package used to make this document is
   `r packageVersion("numDeriv")`.

 * The version of the `Matrix` package used to make this document is
   `r packageVersion("Matrix")`.

 * The version of the `parallel` package used to make this document is
   `r packageVersion("parallel")`.

 * The version of the `kableExtra` package used to make this document is
   `r packageVersion("kableExtra")`.

 * The version of the `Hmisc` package used to make this document is
   `r packageVersion("Hmisc")`.

 * The version of the `data.table` package used to make this document is
   `r packageVersion("data.table")`.

 * The version of the `flextable` package used to make this document is
   `r packageVersion("flextable")`.

 * The version of the `mcmc` package used to make this document is
   `r packageVersion("mcmc")`.

Attach packages.
```{r package}
library("aster")
library("numDeriv")
library("Matrix")
library("parallel")
options("mc.cores" = detectCores())
library("kableExtra")
suppressMessages(library("Hmisc"))
suppressMessages(library("flextable"))
library("data.table")
library("mcmc")
```

Need at least version 1.3-4 of R package `aster` for R generic function `vcov`
to work on results of calls to R functions `aster` and `reaster`.
```{r aster-version}
stopifnot(compareVersion(as.character(packageVersion("aster")), "1.3-4") >= 0)
```
This version is now on CRAN (https://cran.r-project.org/package=aster)
so can be downloaded using R function
`install.packages` or the equivalent by mousing around in the menus
of some app.

A later version 1.3-5 was required by CRAN to fix some test output, but did not
affect any functions in the package or their documentation.  So we do not
need that version.  It was for CRAN's benefit only.

```{r checkerrors, echo=FALSE}
# make all code chunks after this one have option error=TRUE
# knitr::opts_chunk$set(error=TRUE)
```

# Data

## Files

```{r key.gc, echo=FALSE}
key.data <- tools::md5sum("mf.rda")
```

For the analyses here. the data files are
```{r data.input}
load("mf.rda")
ls()
sapply(data.primary, class)
```
for

 * Conard Environmental Research Area (CERA),

 * Grey Cloud Dunes Scientific and Natural Area, and

 * Kellogg-Weaver Dunes, also called McCarthy Lake,

respectively.  These files include the same data on the same individuals
as in the data files used by @kulbaba-et-al and @zenodo but also include
more individuals, who are offspring of those analyzed before.
For more details, see @kulbaba-et-al.

Much preprocessing of these data has already been done.  See
the file `fixup-data.pdf` in this repository.

## Structure

We do aster analyses with random effects (@aster2, @reaster) for
an aster model with graph
$$
\begin{CD}
  1 @>\text{Ber}>> \texttt{Germ}
  @>\text{Ber}>> \texttt{flw}
  @>\text{Poi}>> \texttt{total.pods}
  @>\text{samp}>> \texttt{total.pods.collected}
  @>\text{Poi}>> \texttt{totalseeds}
\end{CD}
$$
where the variables are

 * `Germ` is germination indicator (0 = no, 1 = yes), conditionally Bernoulli.

 * `flw` is survival to flowering (0 = no, 1 = yes),
    conditionally Bernoulli.

 * `total.pods` is total number of pods produced,
   conditionally Poisson.

 * `total.pods.collected` is number of pods collected,
   conditionally Bernoulli (i.e. each pod may be collected or not).
   The arrow leading to this node
   is a subsampling arrow.  The number of pods collected is
   a random sample of the pods produced.

 * `totalseeds` is total number of seeds counted from collected pods,
   conditionally Poisson.

As always with aster models, the name of the distribution for an arrow
is the name of the conditional distribution of the successor variable
given the predecessor variable.
The arrow labeled `samp` is a subsampling arrow.  It is a Bernoulli
arrow but the sampling is experimental rather than biological.
This arrow may be missing in some analyses
(Section \@ref(alternative-structure) below).

Set graphical model description in R.
```{r graph}
vars <- c("Germ", "flw", "total.pods", "total.pods.collected", "totalseeds")
pred <- c(0, 1, 2, 3, 4)
fam <- c(1, 1, 2, 1, 2)
```

## Alternative Structure

Get years.
```{r years}
years <- sort(unique(data.primary[[1]]$year))
names(years) <- years
years

years.offspring <- with(data.primary[[1]], year[cohort == "field"]) |>
    sort() |> unique()
names(years.offspring) <- years.offspring
years.offspring
```

And for each site-year combination, subset data for parents
(`cohort == "greenhouse"`) and check for presence of subsampling.
```{r check.subsampling.parents}
samp.parents <- lapply(data.primary, function(x) lapply(years, function(y) {
    subdat <- subset(x, year == y & cohort == "greenhouse")
    subdat <- droplevels(subdat)
    with(subdat, any(total.pods > total.pods.collected))
    }))
samp.parents
```
We see that we have two parental generation analyses (out of nine) in which
there is no subsampling.  For these we have to change the aster graph for
individuals to omit the subsampling arrow.
```{r graph.no.subsamp}
vars.no.samp <- c("Germ", "flw", "total.pods", "totalseeds")
pred.no.samp <- c(0, 1, 2, 3)
fam.no.samp <- c(1, 1, 2, 2)
```

Now do the same check for offspring
(`cohort == "field"`).
```{r check.subsampling.offspring}
samp.offspring <- lapply(data.primary, function(x)
    lapply(years.offspring, function(y) {
    subdat <- subset(x, year == y & cohort == "field")
    subdat <- droplevels(subdat)
    with(subdat, any(total.pods > total.pods.collected))
    }))
samp.offspring
```
So no issues with this for offspring: all offspring generation analyses
have subsampling.

# Analyses with R Function Reaster {#reaster}

## Parents

We have nine analyses to do here, one for each site-year combination.

We have three covariates: sire, dam, and block, which we make random effects.
We `cbind` the model matrices for sire and dam, so they share a variance
component.
```{r reaster.parents, cache=TRUE, cache.extra=key.data}
rout.parents <- mclapply(data.primary, function(x) mclapply(years, function(y) {
    subdat <- subset(x, year == y & cohort == "greenhouse")
    subdat <- droplevels(subdat)
    has.subsamp <- with(subdat, any(total.pods > total.pods.collected))
    if (has.subsamp) {
        redata <- reshape(subdat, varying = list(vars), direction = "long",
            timevar = "varb", times = as.factor(vars), v.names = "resp")
    } else {
        redata <- reshape(subdat, varying = list(vars.no.samp),
            direction = "long", timevar = "varb",
            times = as.factor(vars.no.samp), v.names = "resp")
    }
    redata <- transform(redata,
        fit = as.numeric(grepl("totalseeds", as.character(varb))),
        root = 1)
    modmat.sire <- model.matrix(~ 0 + fit:paternalID, redata)
    modmat.dam <- model.matrix(~ 0 + fit:maternalID, redata)
    modmat.siredam <- cbind(modmat.sire, modmat.dam)
    if (has.subsamp) {
        reaster(resp ~ fit + varb,
            list(parental = ~ 0 + modmat.siredam, block = ~ 0 + fit:block),
            pred, fam, varb, id, root, data = redata)
    } else {
        reaster(resp ~ fit + varb,
            list(parental = ~ 0 + modmat.siredam, block = ~ 0 + fit:block),
            pred.no.samp, fam.no.samp, varb, id, root, data = redata)
    }
    }, mc.preschedule = FALSE), mc.preschedule = FALSE)
```

Let's see what we got.
Check what we got.
```{r quid.est.happening}
lapply(rout.parents, function(x) sapply(x, function(x) inherits(x, "reaster")))
```

Any variance components estimated to be zero?
```{r quid.est.happening.too}
lapply(rout.parents, function(x) sapply(x, function(x) x$nu))
```
One case where block effects are estimated to be zero,
but all cases have parental effects estimated to be nonzero,
which justifies further consideration of genetic effects.

## Offspring

We have six analyses to do here, one for each site-year combination
(in which there are offspring).
```{r reaster.offspring, cache=TRUE, cache.extra=key.data}
rout.offspring <- mclapply(data.primary,
    function(x) mclapply(years.offspring, function(y) {
    subdat <- subset(x, year == y & cohort == "field")
    subdat <- droplevels(subdat)
    redata <- reshape(subdat, varying = list(vars), direction = "long",
        timevar = "varb", times = as.factor(vars), v.names = "resp")
    redata <- transform(redata,
        fit = as.numeric(grepl("totalseeds", as.character(varb))),
        root = 1)
    reaster(resp ~ fit + varb,
        list(parental = ~ 0 + fit:grandpaternalID, block = ~ 0 + fit:block),
        pred, fam, varb, id, root, data = redata)
    }))
```

Let's see what we got.
Check what we got.
```{r quid.est.happening.offspring}
lapply(rout.offspring, function(x)
    sapply(x, function(x) inherits(x, "reaster")))
```

Any variance components estimated to be zero?
```{r quid.est.happening.too.offspring}
lapply(rout.offspring, function(x) sapply(x, function(x) x$nu))
```
No case where any variance components are estimated to be zero.

# Asymptotic Variance-Covariance Matrices of Estimates

## Parents

```{r vcov.parents, cache=TRUE, dependson="reaster.parents"}
vcov.parents <- mclapply(rout.parents, function(x)
    mclapply(x, vcov, complete = TRUE, re.too = TRUE,
    standard.deviation = FALSE))
```

## Offspring

```{r vcov.offspring, cache=TRUE, dependson="reaster.offspring"}
vcov.offspring <- mclapply(rout.offspring, function(x)
    mclapply(x, vcov, complete = TRUE, re.too = TRUE,
    standard.deviation = FALSE))
```

# Mapping Sire and Grandsire Effects to Mean Values

## A Function Factory

We follow Section 9 of @zenodo *mutatis mutandis*.  The main changes are
here we will have a vectorizing function that simultaneously does
mean fitness values for a specified set of individuals.

We follow the dictates of literate programming
([Wikipedia page](https://en.wikipedia.org/wiki/Literate_programming))
developing our function a little bit at a time and then assembling all
the bits into the whole function.  The function we are developing will have
one argument `rout`, so we make an object of the correct type to exercise
our code bits on.
```{r map-rout}
rout <- rout.parents[[1]][[1]]
```

Our first bit of code does some error checking and setup.
```{r map-setup}
    stopifnot(inherits(rout, "reaster"))
    aout <- rout$obj
    stopifnot(inherits(aout, "aster"))
    nnode <- ncol(aout$x)
    nind <- nrow(aout$x)
    fixed <- rout$fixed
    random <- rout$random
```

The next job is to figure out the subsampling nodes of the graph (if
present).  Here the answer is the fourth of five.
```{r map-subsampling}
    if (nnode == 4) {
        is.subsamp <- rep(FALSE, 4)
    } else if (nnode == 5) {
        is.subsamp <- c(FALSE, FALSE, FALSE, TRUE, FALSE)
    } else stop("can only deal with graphs for individuals with 4 or 5 nodes",
        "\nand graph is linear, and subsampling arrow is 4th of 5")
```

Now we have to deal with the problem that the only R function we have that
maps from canonical to mean value parameters is R function `predict.aster`
and we need to give it an object of class `"aster"`.  So we make one "by hand"
rather than by invocation of R function `aster`.  The simplest way to do that
is to start with an already existing object of class `"aster"` that was
produced by R function `aster` (which is found in our object
of class `"reaster"`) and modify it as needed.
Here we only want the random effects that have the string `"paternalID"`
in their names (that is, either paternalID or grandpaternalID).
```{r map-fake}
    # fake object of class aster
    randlab <- unlist(lapply(rout$random, colnames))
    include.random <- grepl("paternalID", randlab, fixed = TRUE)
    fake.out <- aout
    fake.beta <- with(rout, c(alpha, b[include.random]))
    modmat.random <- Reduce(cbind, random)
    stopifnot(ncol(modmat.random) == length(rout$b))
    # never forget drop = FALSE in programming R
    modmat.random <- modmat.random[ , include.random, drop = FALSE]
    fake.modmat <- cbind(fixed, modmat.random)
    # now have to deal with objects of class aster (as opposed to reaster)
    # thinking model matrices are three-way arrays.
    stopifnot(prod(dim(aout$modmat)[1:2]) == nrow(fake.modmat))
    fake.modmat <- array(as.vector(fake.modmat),
        dim = c(dim(aout$modmat)[1:2], ncol(fake.modmat)))
    fake.out$modmat <- fake.modmat
```

The next job is to figure out which parameters are which.
```{r map-extractors}
    nparm <- length(rout$alpha) + length(rout$b) + length(rout$nu)
    is.alpha <- 1:nparm %in% seq_along(rout$alpha)
    is.bee <- 1:nparm %in% (length(rout$alpha) + seq_along(rout$b))
    is.nu <- (! (is.alpha | is.bee))
```

The next job is to figure out which individuals are in which "families"
(paternalID) and to get one of each for the output vector.
```{r map-which-ind}
    # figure out individuals from each family
    m <- rout$random$parental
    dads <- grep("paternal", colnames(m))
    # get family, that is, paternalID or grandpaternalID as the case may be
    fams <- colnames(m)[dads] |> sub("^.*ID", "", x = _)
    # drop maternal effects columns (if any)
    m.dads <- m[ , dads, drop = FALSE]
    # make into 3-dimensional array, like obj$modmat
    m.dads <- array(m.dads, c(nind, nnode, ncol(m.dads)))
    # only keep fitness node
    # only works for linear graph
    m.dads <- m.dads[ , nnode, ]
    # redefine dads as families of individuals
    stopifnot(as.vector(m.dads) %in% c(0, 1))
    stopifnot(rowSums(m.dads) == 1)
    # tricky, only works because each row of m.dads
    # is indicator vector of family,
    # so we are multiplying family number by zero or one
    dads <- drop(m.dads %*% as.integer(fams))
    # find one individual in each family
    sudads <- sort(unique(dads))
    which.ind <- match(sudads, dads)
```

That finishes the setup.  Now we write the result of our factory function,
which is another function with a single argument `alphabeenu` which is the
vector of all the variables (fixed effects, random effects, and variance
components.
```{r map-alphabeenu}
alphabeenu <- with(rout, c(alpha, b, nu))
```

So first check that this argument is OK.
```{r map-return-check}
        stopifnot(is.numeric(alphabeenu))
        stopifnot(is.finite(alphabeenu))
        stopifnot(length(alphabeenu) == nparm)
```
Then extract the various parts.  And make the `coefficients` component
of our fake object of class `"aster"` to be the fixed effects vector `alpha`
plus the random effects vector of the random effects we are "predicting"
(those indicated by the indicator vector `include.random`).
```{r map-return-extract}
        alpha <- alphabeenu[is.alpha]
        bee <- alphabeenu[is.bee]
        nu <- alphabeenu[is.nu]
        fake.beta <- c(alpha, bee[include.random])
        fake.out$coefficients <- fake.beta
```

Do the prediction, mapping unconditional submodel canonical parameters $\beta$
to conditional mean value parameters $\xi$.
```{r map-return-predict}
        pout <- predict(fake.out, model.type = "conditional",
            is.always.parameter = TRUE)
```

Then put the result $\xi$ into matrix form, rows are individuals, columns
are nodes of the graph for individuals.  Toss the column of subsampling nodes
(if any) and multiply the rest to get unconditional mean value parameters for
fitness (mean fitness).  Put in a comment that this only works for this
particular graph.
```{r map-return-moo}
        xi <- matrix(pout, ncol = nnode)
        xi <- xi[ , ! is.subsamp, drop = FALSE]
        mu <- apply(xi, 1, prod)
```

Finally, we put names on `mu` and return only one for each "family"
(paternalID).
```{r map-return-subset}
        mu <- mu[which.ind]
        names(mu) <- paste0("PID",
            formatC(sudads, format="d", width=3, flag="0"))
```

Now put it all together in a function factory.
```{r map}
map.factory <- function(rout) {
<<map-setup>>
<<map-subsampling>>
<<map-fake>>
<<map-extractors>>
<<map-which-ind>>
    function(alphabeenu) {
<<map-return-check>>
<<map-return-extract>>
<<map-return-predict>>
<<map-return-moo>>
<<map-return-subset>>
        return(mu)
    }
}
```

R function `map.factory` has one argument, an object of class `reaster`
produced by a call to R function `reaster` and produces a function with
one argument, which is a vector $\theta = (\alpha, b, \nu)$ that contains
the variables (fixed effects, standardized random effects, and standard
deviation components), and that produced function returns
mean fitness estimates for one individual from each "family" (sire for
parental fits, grandsire for offspring fits).
That is, it is a function whose value is another function.

**Warning:** This function only works for linear aster graphs for "individuals"
(in scare quotes) having four or five arrows and the only subsampling is the
fourth of five.

Try it out.
```{r fitness-try-out, error=TRUE}
map <- map.factory(rout)
map(alphabeenu)
```

## Apply To Parents {#delta}

In hindsight, we did not want to apply R generic function `vcov`
[as we did above](#asymptotic-variance-covariance-matrices-of-estimates)
to make a separate list.  (We could use R function `mapply`
to work on multiple parallel lists, but don't choose to.)
So we invoke R function `vcov` again here, putting estimates and their
variance-covariance matrices in the same list.
```{r moo.parents, cache=TRUE, dependson="reaster.parents"}
moo.parents <- mclapply(rout.parents, function(x) mclapply(x, function(y) {
    map <- map.factory(y)
    alphabeenu <- with(y, c(alpha, b, nu))
    estimates <- map(alphabeenu)
    jack <- jacobian(map, alphabeenu)
    vcov.base <- vcov(y, complete = TRUE, re.too = TRUE,
        standard.deviation = FALSE)
    vcov.estimates <- jack %*% vcov.base %*% t(jack)
    return(list(estimates = estimates, vcov = vcov.estimates))
}))
```

In the above code we are applying the delta method for differentiable
functions of asymptotically normal estimators.  If $\hat{\theta}$ is
asymptotically normal with variance given by R function `vcov` and $g$
is a differentiable function, then the asymptotic variance of $g(\hat{\theta})$
is given by
$$
   J \mathop{\rm vcov}(\hat{\theta}) J^T
$$
which is computed by the line
```
    vcov.estimates <- jack %*% vcov.base %*% t(jack)
```
in the code above.  And we do not calculate the derivative matrix, also
called Jacobian matrix, by calculus, but rather let the computer do it for
us (numerically) using R function `jacobian` from R package `numDeriv`.

If you want to replace the function being used here (the $g$ we are talking
about is here implemented by R function `map`) by some other function, just
do it.  The delta method is valid for any differentiable function.

You replace R function `map` in both places it occurs (in computing R vector
`estimates` and in computing R matrix `jack`) with your R function that
calculates some other estimator.

## Apply To Offspring

Same for offspring.
```{r moo.offspring, cache=TRUE, dependson="reaster.offspring"}
moo.offspring <- mclapply(rout.offspring, function(x) mclapply(x, function(y) {
    map <- map.factory(y)
    alphabeenu <- with(y, c(alpha, b, nu))
    estimates <- map(alphabeenu)
    jack <- jacobian(map, alphabeenu)
    vcov.base <- vcov(y, complete = TRUE, re.too = TRUE,
        standard.deviation = FALSE)
    vcov.estimates <- jack %*% vcov.base %*% t(jack)
    return(list(estimates = estimates, vcov = vcov.estimates))
}))
```

# Genetic Change in Mean Fitness Due to Selection {#fftns-change}

## Definition

One of the primary quantities of scientific interest, and the other quantities
we calculate are related to it, is the part of the change in mean fitness
in one generation that Fisher's fundamental theorem (FFTNS) addresses.

This quantity is very simple in some respects and a bit tricky in other
respects.  We start with the "breeder's equation" (also called
the Robertson-Price equation).  What does natural selection do?  It changes
frequencies of genotypes (or phenotypes) from what they were before selection
to the same thing multiplied by fitness normalized to be a probability vector,
that is, if $\theta$ are the frequencies before selection and $\mu$ is
(expected) fitness, then
$$
   \theta \cdot \frac{\mu}{\text{sum}(\mu)}
$$
are the frequencies after selection.  And the change in fitness is
$$
   \theta \cdot \frac{\mu}{\text{sum}(\mu)} - \theta
$$
and the change in mean fitness is the mean of the above
\begin{equation}
   \mathop{\rm sum}\left(\theta \cdot \frac{\mu}{\mathop{\rm sum}(\mu)}\right)
   -
   \mathop{\rm mean}(\theta)
   (\#eq:breeder-zero)
\end{equation}
which can also be written
\begin{equation}
   \text{mean}\left(\theta \cdot \frac{\mu}{\text{mean}(\mu)}\right) -
   \text{mean}(\theta)
   (\#eq:breeder-one)
\end{equation}
(the difference between `sum` and `mean` is a factor of `n` (the length of
the vectors $\theta$ and $\mu$) and this cancels in the numerator and
denominator in going from one to the other.

## Equivalent Definitions

This does not look like all treatments of the breeder's equation.  Often
one sees the covariance operator used.  For any random variables $X$ and $Y$
$$
   \mathop{\rm cov}(X, Y) = E(X Y) - E(X) E(Y)
$$
and in the special case $E(Y) = 1$ this becomes
$$
   \mathop{\rm cov}(X, Y) = E(X Y) - E(X)
$$
and in \@ref(eq:breeder-one) we have arranged that the random variable
$Y = \mu / \text{mean}(\mu)$ does have mean one by the simple expedient
of dividing $\mu$ by its mean.  Thus \@ref(eq:breeder-one) is equivalent to
\begin{equation}
   \text{cov}\left(\theta, \frac{\mu}{\text{mean}(\mu)}\right)
   (\#eq:breeder-two)
\end{equation}
This is shorter, but we think it obscures the logic.  For those who like
their math mysterious, \@ref(eq:breeder-two) is better.

Equation \@ref(eq:breeder-two) can be made even more mysterious by introducing
the term *relative fitness* for $\mu / \mathop{\rm mean}(\mu)$ so it becomes
\begin{equation}
   \text{cov}\left(\text{trait}, \text{relative fitness}\right)
   (\#eq:breeder-too-too)
\end{equation}

## Weighted Averages

We think it is less obscure to go back to \@ref(eq:breeder-zero) and introduce
the notion of *weighted average*.   A *weighted average* of a vector $\theta$
is
\begin{equation}
   \sum\nolimits_i \theta_i p_i
   (\#eq:weighted-average)
\end{equation}
where $p$ is a probability vector, that is, its components $p_i$ are
nonnegative and sum to one.  A simple average is the special case where
$p_i = 1 / n$ for all $i$ where $n$ is the length of $\theta$ and $p$.

Another helpful concept is that of *unnormalized weights*.  If $\mu$ is
a vector with nonnegative components, then $\mu / \mathop{\rm sum}(\mu)$
is a probability vector.  When we use this probability vector to make
a weighted average, we call $\mu$ the *unnormalized* weight vector and
$\mu / \mathop{\rm sum}(\mu)$ the *normalized* weight vector.

So \@ref(eq:breeder-zero) is just the difference between a weighted average
of $\theta$ and a simple average of $\theta$ with $\mu$ as the unnormalized
weight vector.

## The Tricky Bit

Now we want to apply this logic where the trait $\theta$ under discussion
is fitness itself.  So we set $\theta = \mu$ giving
\begin{equation}
   \mathop{\rm mean}\left(\mu \cdot \frac{\mu}{\text{mean}(\mu)}\right) -
   \text{mean}(\mu)
   (\#eq:breeder-moo)
\end{equation}

## Too Long, Didn't Read

Whether one regards all of the above as trivial, deep, clear, or confusing,
it is the only bit of math we know of that addresses the question.
@lande-arnold discuss three different ways to express the same concept under
certain assumptions (which are further discussed in @lande-arnold-tr) but
this does not change the concept that \@ref(eq:breeder-one) expresses.
Or \@ref(eq:breeder-moo) expresses.

In short, for each pedigreed cohort, we have obtained a vector $\mu$
returned by R function `map`, which estimates family-specific mean
absolute fitnesses.  To estimate overall mean fitness, we obtain a
weighted sum of these $\mu$, using as weights, the relative fitness
of each family, obtained by dividing $\mu$ by $\mathop{\rm mean}(\mu)$.
We obtain the change in mean fitness by subtracting the mean fitness
before selection, when families were equally represented.

## Application

We make this into a function so we can differentiate it numerically.
```{r change}
fitness_change <- function(mu) mean(mu * (mu / mean(mu) - 1))
```

### Parents {#delta-grad}

Now we apply this function to parents
```{r change.parents, cache=TRUE, dependson="moo.parents"}
change.parents <- mclapply(moo.parents, function(x) mclapply(x, function(y) {
    grad <- grad(fitness_change, y$estimates)
    my.vcov <- t(grad) %*% y$vcov %*% grad
    return(list(estimates = fitness_change(y$estimates), vcov = my.vcov))
}))
```

Here we do the delta method slightly differently than in Sections \@ref(delta)
above and \@ref(r-function) below where we use R function `jacobian` from R
package `numDeriv`.  Here (for no particular reason) we use R function `grad`
from the same package.  R function `jacobian` differentiates vector-to-vector
functions.  R function `grad` differentiates vector-to-scalar functions.
When both are applied to a vector-to-scalar function (which is OK because
R cannot tell the difference between scalars and vectors of length one),
`jacobian` produces a matrix with one row and `grad` produces a vector.
This means we put the transpose in a different place when calculating `my.cov`.
If we had used R function `jacobian` instead, we could have made this
code look like our other examples.

### Offspring

And offspring
```{r change.offspring, cache=TRUE, dependson="moo.offspring"}
change.offspring <- mclapply(moo.offspring, function(x)
    mclapply(x, function(y) {
    grad <- grad(fitness_change, y$estimates)
    my.vcov <- t(grad) %*% y$vcov %*% grad
    return(list(estimates = fitness_change(y$estimates), vcov = my.vcov))
}))
```

## Tables

First we map site keys to site names.
```{r translate}
site.translate <- c(CS = "CERA", GC = "Grey Cloud Dunes", KW = "McCarthy Lake")
```

Turn the calculations above into tables
```{r doit}
doit <- function(w, cap, lab) {
    e <- lapply(w, function(x) lapply(x, function(y) y$estimates)) |> unlist()
    se <- lapply(w, function(x) lapply(x, function(y) y$vcov)) |> unlist() |>
        sqrt()
    foo <- cbind(estimates = e, std.err. = se)
    site <- substr(rownames(foo), 1, 2)
    site.cs <- match("CS", site)
    site.gc <- match("GC", site)
    site.kw <- match("KW", site)
    year <- substr(rownames(foo), 4, 7)
    rownames(foo) <- year
    kbl(foo, caption = cap, label = lab,
        format = "latex", escape = FALSE, booktabs = TRUE, digits = 4) |>
        kable_styling() |>
        pack_rows(site.translate["GC"], min(site.gc), max(site.gc),
            indent = FALSE) |>
        pack_rows(site.translate["KW"], min(site.kw), max(site.kw),
            indent = FALSE) |>
        pack_rows(site.translate["CS"], min(site.cs), max(site.cs),
            indent = FALSE)
}
```

The following makes Table \@ref(tab:changeParents) below.
```{r change-table-parents}
doit(change.parents,
"Intragenerational Change in Mean Fitness Due to Selection on Parental Cohort",
    "changeParents")
```

The following makes Table \@ref(tab:changeOffspring) below.
```{r change-table-offspring}
doit(change.offspring,
"Intragenerational Change in Mean Fitness Due to Selection on Offspring Cohort",
    "changeOffspring")
```

# Comparison of Mean Fitness in Parents and Offspring

## Re-arrange Data

Now we want to make inter-dataset comparisons and inter-year comparisons.
To continue with functional programming we need to temporarily step outside
of functional programming and use some plain old for loops to re-assemble
the data the way we want it.  All of our comparisons in this section will
involve "parents" in one year and "parents" in the following year (these
are pedigreed individuals, and individuals planted in one year have full
sibs planted in the next.  The "offspring" are open pollinated individuals
(mothers known, fathers unknown) whose mothers are individuals among the
"parents" in the previous year.

We collect each of these into one vector with one variance-covariance
matrix so we can apply one function to them to get estimates and differentiate
that one function numerically to apply the delta method to get standard
errors.
```{r re-arrange,cache=TRUE,dependson=c("change.parents","change.offspring")}
redata <- list()
for (site in names(moo.offspring)) {
    for (year in names(moo.offspring[[site]])) {
        year.before <- as.character(as.numeric(year) - 1)
        moo.prev <- moo.parents[[site]][[year.before]]
        moo.par <- moo.parents[[site]][[year]]
        stopifnot(names(moo.prev$estimates) == names(moo.par$estimates))
        moo.off <- moo.offspring[[site]][[year]]
        stopifnot(names(moo.off$estimates) %in% names(moo.prev$estimates))
        idx <- match(names(moo.off$estimates), names(moo.prev$estimates))
        my.off.est <- rep(0, length(moo.prev$estimates))
        my.off.vcov <- matrix(0, length(my.off.est), length(my.off.est))
        my.off.est[idx] <- moo.off$estimates
        my.off.vcov[idx, idx] <- moo.off$vcov
        redata[[site]][[year]] <- list( estimates =
            c(moo.prev$estimates, moo.par$estimates, my.off.est),
            vcov = bdiag(moo.prev$vcov, moo.par$vcov, my.off.vcov))
    }
}
```

We say that offspring are in the same "family" as parental individuals
(pedigreed crosses, whether parents of these offspring or not) if the
grandsire of the offspring (the maternal grandsire since sires of offspring
are unknown) is the same as the sire of a parental (pedigreed) individual.
The code in the function involving R vector `idx` is there to make the
estimates and variance-covariance matrices for offspring match those for
"parents".  We have estimates for each "family" regardless of whether that
"family" occurs in the offspring (was planted in the offspring experiment)
or not.  Fitness is "estimated" to be zero for those families that do not
occur in the data (were not planted).

## Functions of Interest

We calculate four functions of interest.

  * total change in mean fitness (parents to offspring)
    \begin{equation}
    \bar{\mu}_\text{total} =
    \text{mean}\left(\mu_\text{off} \cdot
    \frac{\mu_\text{prev}}{\text{mean}(\mu_\text{prev})}\right)
    - \text{mean}(\mu_\text{prev})
    (\#eq:total)
    \end{equation}
    offspring family mean fitness weighted by relative fitness of their
    family in the parental generation
    (because in the experiment offspring were not planted according to
    their frequency, so we must correct for that)

Which is divided into

  * change due to selection (what Fisher's fundamental theorem of
    natural selection is supposed to predict) in the parents
    given by \@ref(eq:breeder-moo) above.  In this case
\begin{equation}
   \mathop{\rm mean}\left(\mu_\text{prev} \cdot \frac{\mu_\text{prev}}{\mathop{\rm mean}(\mu_\text{prev})}\right) -
   \mathop{\rm mean}(\mu_\text{prev})
   (\#eq:breeder-moo-prev)
\end{equation}

  * environmental change
    \begin{equation}
    \bar{\mu}_\text{environment} =
    \text{mean}(\mu_\text{par}) - \text{mean}(\mu_\text{prev})
    (\#eq:environment)
    \end{equation}
    difference between pedigreed individuals (which are full sibs) in different
    years.  (Of course, this includes some genetic change because full sibs
    are not clones.  But it does unbiasedly estimate the change due to different
    environments in the two years because the individuals are random samples
    from their pedigreed families.)

  * residual change, the part of the total change not due to environment
    or selection
    \begin{equation}
    \bar{\mu}_\text{residual} =
    \bar{\mu}_\text{total} -
    \bar{\mu}_\text{selection} -
    \bar{\mu}_\text{environment}
    (\#eq:residual)
    \end{equation}
    This may involve recombination, gene-environment interaction,
    gene-gene interaction or anything else not accounted for in
    additive effects of selection and environment.

## R Function

Here is our function to compute estimates of interest.
```{r foo}
foo <- function(theta) {
    stopifnot(is.atomic(theta))
    stopifnot(is.vector(theta))
    stopifnot(is.numeric(theta))
    stopifnot(is.finite(theta))
    stopifnot(length(theta) %% 3 == 0)
    nmu <- length(theta) %/% 3
    is.prev <- seq_along(theta) <= nmu
    is.off <- seq_along(theta) > 2 * nmu
    is.par <- (! (is.prev | is.off))
    mu.prev <- theta[is.prev]
    mu.par <- theta[is.par]
    mu.off <- theta[is.off]
    
    delta.total <- sum(mu.off * mu.prev) / sum(mu.prev) - mean(mu.prev)
    delta.environ <- mean(mu.par - mu.prev)
    delta.fftns <- mean((mu.prev / mean(mu.prev) - 1) * mu.prev)
    delta.non.fftns <- delta.total - delta.environ - delta.fftns
    c(total = delta.total, environmental = delta.environ,
        selection = delta.fftns, residual = delta.non.fftns)
}
```

Try it out.
```{r foo.try}
e <- redata[[1]][[1]]$estimates
foo(e)
```

So now we want to use this function to calculate estimates and standard
errors.
```{r foo.doit,cache=TRUE,dependson="re-arrange"}
fout <- mclapply(redata, function(x) mclapply(x, function(y) {
    e <- foo(y$estimates)
    jack <- jacobian(foo, y$estimates)
    my.vcov <- jack %*% y$vcov %*% t(jack)
    list(estimates = e, std.err. = sqrt(as.vector(diag(my.vcov))))
}))
```

Our use of the delta method here is just like in Section \@ref(delta)
above, where it was explained.

Look at one, just to see we are OK.
```{r foo.doit.look}
fout[[1]][[2]]
```

Now make a table for this.
```{r footab}
doit.too <- function(w) {
    e <- lapply(w, function(x) lapply(x, function(y) y$estimates)) |> unlist()
    se <- lapply(w, function(x) lapply(x, function(y) y$std.err.)) |> unlist()
    foo <- cbind(estimates = e, std.err. = se)
    site <- substr(rownames(foo), 1, 2)
    site.cs <- "CS" == site
    site.gc <- "GC" == site
    site.kw <- "KW" == site
    year <- substr(rownames(foo), 4, 7)
    year.2016 <- "2016" == year
    year.2017 <- "2017" == year
    kind <- substr(rownames(foo), 9, 100)
    rownames(foo) <- kind
    kbl(foo, caption = "Estimates of Change in Mean Fitness", label = "decomp",
        format = "latex", escape = FALSE, booktabs = TRUE, digits = 4) |>
        kable_styling() |>
        pack_rows(paste(site.translate["GC"], "2015-2016"),
            min(which(site.gc & year.2016)),
            max(which(site.gc & year.2016)),
            indent = FALSE) |>
        pack_rows(paste(site.translate["GC"], "2016-2017"),
            min(which(site.gc & year.2017)),
            max(which(site.gc & year.2017)),
            indent = FALSE) |>
        pack_rows(paste(site.translate["KW"], "2015-2016"),
            min(which(site.kw & year.2016)),
            max(which(site.kw & year.2016)),
            indent = FALSE) |>
        pack_rows(paste(site.translate["KW"], "2016-2017"),
            min(which(site.kw & year.2017)),
            max(which(site.kw & year.2017)),
            indent = FALSE) |>
        pack_rows(paste(site.translate["CS"], "2015-2016"),
            min(which(site.cs & year.2016)),
            max(which(site.cs & year.2016)),
            indent = FALSE) |>
        pack_rows(paste(site.translate["CS"], "2016-2017"),
            min(which(site.cs & year.2017)),
            max(which(site.cs & year.2017)),
            indent = FALSE)
}
doit.too(fout)
```
The code chunk above makes Table \@ref(tab:decomp) on this page.

# Plotting the Decomposition

We are going to try to show these numbers and standard errors in a plot.
We will just do one example in this section.
```{r plot-site-year}
site <- "GC"
year <- "2016"
year.before <- as.character(as.numeric(year) - 1)
f <- fout[[site]][[year]]
names(f)
```

Critical value.
```{r plot-crit}
conf.level <- 0.95
crit <- qnorm((1 + conf.level) / 2)
```

The horizontal length of the arrows in this plot are entirely meaningless.
They have to be different because we don't want the error bars to overlap.
```{r plot-caption}
cap <- paste0("Estimates in Table 3 with Error Bars.",
   "  Horizontal coordinate is meaningless, but short vectors add up",
   " to long vector.",
   "  Vertical bars are approximate 95% confidence intervals.",
   "  Site is ", site.translate[site], ".",
   "  Total is difference of parents (", year.before, ") and offspring (",
   year, ").",
   "  Selection is difference due to selection in parents (", year.before, ").",
   "  Environmental is difference due to different environments in the",
   " two years: difference of parents (", year.before, ") and full sibs grown",
   " along with offspring (", year, ").",
   "  Residual is Total $-$ Selection $-$ Environmental, mostly genetic change",
   " not due to selection in parents or gene-environment or gene-gene",
   " interaction.")
```
```{r plot, fig.align="center", fig.cap=cap}
par(mar = c(1, 4, 0, 0) + 0.1)
xlength <- c(3, 0.9, 1.0, 1.1)
errbar(x = xlength, y = f$estimates,
   yplus = f$estimates + crit * f$std.err.,
   yminus = f$estimates - crit * f$std.err.,
   axes = FALSE, xlim = c(0, 3),
   ylab = "change in mean fitness",
   pch = NA_integer_)
box()
axis(side = 2)
arrows(x0 = rep(0, 4), x1 = xlength, y0 = rep(0, 4),
   y1 = f$estimates)

# now have to do extraordinarily painful calculation of angles to rotate text
par.pin <- par("pin")
par.plt <- par("plt")
par.usr <- par("usr")
par.plt <- c(diff(par.plt[1:2]), diff(par.plt[3:4]))
par.usr <- c(diff(par.usr[1:2]), diff(par.usr[3:4]))
foo <- par.pin * par.plt / par.usr
foo
angles <- atan(f$estimates / xlength * foo[2] / foo[1]) / pi * 180
angles
radians <- 2 * pi * angles / 360
cos(radians)
sin(radians)
delta.text <- 0.2
x <- xlength
y <- f$estimates
n <- names(y)
for (i in seq_along(f$estimates))
    text(x = x[i] / 2 - sin(radians[i]) * delta.text / foo[1],
    y = y[i] / 2 + cos(radians[i]) * delta.text / foo[2],
    labels = n[i], col = "darkgray", srt = angles[i])
```

The plot made is Figure \@ref(fig:plot) below.
We might want to say that the arrow labeled "selection" occurs earlier
than the other two arrows, but the endpoints of the arrows are not points
in time.  The arrow labeled "total" is the difference in fitness in the
"parents" which takes the entire first year to play out and in the "offspring"
which takes the entire second year to play out.  Ditto for the arrow labeled
"environment".  But the arrow labeled "selection" describes what happens
only in the "parents" which plays out in the first year.  So the arrow
labeled "residual" is even more complicated.

Rather than just continue here with the other plots, we move them to a
[later section](#more-plots).

# Fitness (Rather than Change Therein)

In this section we revisit change in mean fitness \@ref(eq:total) above,
and separately calculate the two mean fitnesses that it is the difference of.
These are the mean fitness of the parental generation
\begin{equation}
   \text{mean}(\mu_\text{prev})
    (\#eq:parent)
\end{equation}
and the mean fitness of the offspring generation
\begin{equation}
    \text{mean}\left(\mu_\text{off} \cdot
    \frac{\mu_\text{prev}}{\text{mean}(\mu_\text{prev})}\right)
    (\#eq:offspring)
\end{equation}

Then we follow the pattern of
Section \@ref(fftns-change) above
*mutatis mutandis*.

## Parents

First we apply \@ref(eq:parent) to all parental experiments (all years,
whether they were followed by offspring or not).
```{r fitness.parents, cache=TRUE, dependson="moo.parents"}
fitness.parents <- mclapply(moo.parents, function(x) mclapply(x, function(y) {
    e <- y$estimates
    grad <- rep(1 / length(e), length(e)) # this is easy calculus
    my.vcov <- t(grad) %*% y$vcov %*% grad
    return(list(estimates = mean(e), vcov = my.vcov))
}))
```

Here we are using R function `grad` in the delta method like we explained
in Section \@ref(delta-grad) above.

We make a table for this like Table \@ref(tab:changeParents).
The table we are making is Table \@ref(tab:fitnessParents).
```{r fitness-parents-table}
doit(fitness.parents, "Mean Fitness of Parental Populations", "fitnessParents")
```

## Offspring

Now we apply \@ref(eq:offspring) to all offspring experiments.
Since this also involves parent data, we are following Section
\@ref(comparison-of-mean-fitness-in-parents-and-offspring) *mutatis mutandis*
for this calculation.

First we redefine R function `foo` above to calculate offspring fitness
```{r fitness-offspring-foo}
foo <- function(theta) {
    stopifnot(is.atomic(theta))
    stopifnot(is.vector(theta))
    stopifnot(is.numeric(theta))
    stopifnot(is.finite(theta))
    stopifnot(length(theta) %% 3 == 0)
    nmu <- length(theta) %/% 3
    is.prev <- seq_along(theta) <= nmu
    is.off <- seq_along(theta) > 2 * nmu
    is.par <- (! (is.prev | is.off))
    mu.prev <- theta[is.prev]
    mu.par <- theta[is.par]
    mu.off <- theta[is.off]
    
    sum(mu.off * mu.prev) / sum(mu.prev)
}
```

Then we repeat Section
\@ref(comparison-of-mean-fitness-in-parents-and-offspring) above
except for using the `foo` defined here rather than the `foo` defined there.
```{r fitness-offspring, cache=TRUE, dependson="re-arrange"}
fitness.offspring <- mclapply(redata, function(x) mclapply(x, function(y) {
    e <- foo(y$estimates)
    jack <- jacobian(foo, y$estimates)
    my.vcov <- jack %*% y$vcov %*% t(jack)
    list(estimates = e, vcov = as.matrix(my.vcov))
}))
```
(We changed the second component of the result from `std.err.` to `vcov`
in order to match the parental calculation.  The reason for the `as.matrix`
is to convert a sparse matrix from R package `Matrix` into an ordinary
(core R) matrix.)

But our table is done by the same code as for the table for
parents in this section.
The table we are making is Table \@ref(tab:fitnessOffspring).
```{r fitness-offspring-table, error=TRUE}
doit(fitness.offspring, "Mean Fitness of Offspring Populations",
    "fitnessOffspring")
```


# More Plots

We ought to use a loop here but R markdown does not seem to be up to the
task.  So we use five more code chunks, which we hide.
```{r plot-setup-GC-2017, echo=FALSE, results="hide"}
site <- "GC"
year <- "2017"
year.before <- as.character(as.numeric(year) - 1)
f <- fout[[site]][[year]]
names(f)
```
```{r plot-caption-GC-2017, echo=FALSE}
<<plot-caption>>
```
```{r plotGC2017, fig.align="center", fig.cap=cap, echo=FALSE, results="hide"}
<<plot>>
```

```{r plot-setup-CS-2016, echo=FALSE, results="hide"}
site <- "CS"
year <- "2016"
year.before <- as.character(as.numeric(year) - 1)
f <- fout[[site]][[year]]
names(f)
```
```{r plot-caption-CS-2016, echo=FALSE}
<<plot-caption>>
```
```{r plotCS2016, fig.align="center", fig.cap=cap, echo=FALSE, results="hide"}
<<plot>>
```

```{r plot-setup-CS-2017, echo=FALSE, results="hide"}
site <- "CS"
year <- "2017"
year.before <- as.character(as.numeric(year) - 1)
f <- fout[[site]][[year]]
names(f)
```
```{r plot-caption-CS-2017, echo=FALSE}
<<plot-caption>>
```
```{r plotCS2017, fig.align="center", fig.cap=cap, echo=FALSE, results="hide"}
<<plot>>
```

```{r plot-setup-KW-2016, echo=FALSE, results="hide"}
site <- "KW"
year <- "2016"
year.before <- as.character(as.numeric(year) - 1)
f <- fout[[site]][[year]]
names(f)
```
```{r plot-caption-KW-2016, echo=FALSE}
<<plot-caption>>
```
```{r plotKW2016, fig.align="center", fig.cap=cap, echo=FALSE, results="hide"}
<<plot>>
```

```{r plot-setup-KW-2017, echo=FALSE, results="hide"}
site <- "KW"
year <- "2017"
year.before <- as.character(as.numeric(year) - 1)
f <- fout[[site]][[year]]
names(f)
```
```{r plot-caption-KW-2017, echo=FALSE}
<<plot-caption>>
```
```{r plotKW2017, fig.align="center", fig.cap=cap, echo=FALSE, results="hide"}
<<plot>>
```

# Write Out Stuff For Paper

## To Do

Table 1 for paper is Table \@ref(tab:fitnessParents) above.
Except we also want post-selection mean fitness too.

Table 2 for paper is Table \@ref(tab:fitnessOffspring) above.

Table 3 for paper is Table \@ref(tab:decomp) above.
Except FFTNS prediction should be included too.
And For that we need to redo the calculation from @zenodo with correct
standard errors (no infinite standard errors because we found the cause of
that (Section \@ref(alternative-structure) above).

Figure 1, six panels, error bars (or something similar) for
selection, environment, residual, total (in that order left-to-right).
No arrows.

## New Table 1

For this we need R object
`fitness.parents` to have the numbers in \@ref(tab:fitnessParents).
and the numbers that are the first term in (\@ref(eq:breeder-moo-prev))
$$
   \mathop{\rm mean}\left(\mu \cdot \frac{\mu}{\mathop{\rm mean}(\mu)}\right)
$$
```{r fitness.parents.after, cache=TRUE, dependson="moo.parents"}
moo.after <- function(mu) mean(mu * mu / mean(mu))
fitness.parents.after <- mclapply(moo.parents, function(x)
    mclapply(x, function(y) {
    e <- moo.after(y$estimates)
    g <- grad(moo.after, y$estimates)
    my.vcov <- drop(t(g) %*% y$vcov %*% g)
    return(list(estimates = mean(e), vcov = my.vcov))
}))

doit.helper <- function(w, cap, lab) {
    e <- lapply(w, function(x) lapply(x, function(y) y$estimates)) |> unlist()
    se <- lapply(w, function(x) lapply(x, function(y) y$vcov)) |> unlist() |>
        sqrt()
    cbind(estimates = e, std.err. = se)
}
foo <- cbind(doit.helper(fitness.parents), doit.helper(fitness.parents.after))
colnames(foo) <- c("e1", "s1", "e2", "s2")
```
Continue with code from R function `doit`.
We have to stop using R package `kableExtra` because it does not work for
Microsoft Word, which this document is not, but we eventually want to get to.
So we switch to R package `flextable`.
```{r ft.align="center", tab.cap="Mean Fitness of Parental Populations", label = "tab1"}
set_flextable_defaults(fonts_ignore=TRUE)
foot <- data.frame(foo) |>
    (\(x) transform(x, site = substr(rownames(x), 1, 2)))() |>
    (\(x) transform(x, year = substr(rownames(x), 4, 7)))() |>
    transform(site = site.translate[site]) |>
    transform(site = as.factor(site)) |>
    as.data.table() |>
    as_grouped_data(groups = c("site"),
        columns = c("year", "e1", "s1", "e2", "s2")) |>
    flextable() |>
    set_header_labels(year = "Year", e1 = "Estimate", s1 = "Std. Error",
        e2 = "Estimate", s2 = "Std. Error") |>
    colformat_double(digits = 4) |>
    add_header_row(colwidths = c(1, 1, 2, 2),
        values = c("", "", "before selection", "after selection"))
foot
```
OK.  Ship out to Microsoft Word.
```{r table.one.to.word}
foot <- set_caption(foot, "Mean Fitness of Parental Populations")
save_as_docx(foot, path = "table1.docx")
```

What a struggle!  And we still have an inexplicable error
(which does not show in the document, only as a compilation warning).

## New Table 2

Just like new table 1 except nothing extra to calculate.
```{r ft.align="center", tab.cap="Mean Fitness of Offspring Populations", label = "tab2"}
foot <- doit.helper(fitness.offspring) |>
    as.data.frame() |>
    (\(x) transform(x, site = substr(rownames(x), 1, 2)))() |>
    (\(x) transform(x, year = substr(rownames(x), 4, 7)))() |>
    transform(site = site.translate[site]) |>
    transform(site = as.factor(site)) |>
    as.data.table() |>
    as_grouped_data(groups = c("site"),
        columns = c("year", "estimates", "std.err.")) |>
    flextable() |>
    set_header_labels(year = "Year", estimates = "Estimate",
        std.err. = "Std. Error") |>
    colformat_double(digits = 4)
foot
```
OK.  Ship out to Microsoft Word.
```{r table.two.to.word}
foot <- set_caption(foot, "Mean Fitness of Offspring Populations")
save_as_docx(foot, path = "table2.docx")
```

## Recalculate Results from Evolution Correction

### Differences Between this Paper Previous Ones

We are going to redo some of the calculations of @zenodo in order to
compare them with this paper.  The approaches are rather different.  Neither
is wrong (except in one detail mentioned presently).  They are just addressing
different (albeit related) things.

In particular, @zenodo calculates the prediction from Fisher's fundamental
theorem of natural selection (FFTNS), which is additive genetic variance for
fitness divided by mean fitness.  In this paper, we do not (at least up to now)
calculate any such thing.  Instead we calculate what this quantity (according
to FFTNS) predicts, actual observed change in mean fitness.  This does not
involve any variance (additive genetic or otherwise).

The error in @zenodo is that some of these quantities had infinite standard
errors, which is a mistake.  That is due solely to not recognizing
(Section \@ref(alternative-structure) above) that some site-year combinations
have subsampling and some do not have it, so we need to do different analyses
(Section \@ref(reaster) above) for the different cases.

### What Is To Be Done

@zenodo, Section 12.3.2 (following @tr696) define
an R function `map.too` that takes one vector argument `balpha` that
decomposes into two parts,

 * a scalar `b` which is the additive genetic effect for a single
   hypothetical individual (not necessarily one in the observed data) and

 * a vector `alpha` which is the fixed effects part of the random effects
   aster model.

To turn this from computer code to math, we let the R function `map.too`
correspond to a mathematical function $f$ of two arguments, so $f(b, \alpha)$
is the value of the function.  Then our FFTNS prediction is
\begin{equation}
   Q(\alpha, \nu) =
   \left(\frac{\partial f(0, \alpha)}{\partial b}\right)^2
   \cdot 4 \nu
   \cdot \frac{1}{f(0, \alpha)}
   (\#eq:fftns-point-estimate)
\end{equation}
where $\nu$ is the variance on the canonical parameter
scale for parental random effects.

We estimate this, of course, by plugging in MLE $Q(\hat{\alpha}, \hat{\nu})$.

To calculate standard errors we need derivatives, which are given by
@zenodo, Section 12.3.2
\begin{align}
   \frac{\partial Q(\alpha, \nu)}{\partial \alpha} & =
   -
   \left(\frac{\partial f(0, \alpha)}{\partial b}\right)^2
   \cdot 4 \nu
   \cdot \frac{1}{f(0, \alpha)^2}
   \cdot \frac{\partial f(0, \alpha)}{\partial \alpha}
   +
   \frac{\partial f(0, \alpha)}{\partial b}
   \cdot 8 \nu
   \cdot \frac{1}{f(0, \alpha)}
   \cdot \frac{\partial^2 f(0, \alpha)}{\partial b \partial \alpha}
   (\#eq:fftns-derivative-alpha)
   \\
   \frac{\partial Q(\alpha, \nu)}{\partial \nu} & =
   \left(\frac{\partial f(0, \alpha)}{\partial b}\right)^2
   \cdot 4
   \cdot \frac{1}{f(0, \alpha)}
   (\#eq:fftns-derivative-nu)
\end{align}

### Another Function Factory

Rather than exactly reproduce what @zenodo do, we combine our approach
in Section \@ref(a-function-factory) above to make a function factory that
produces a function that calculates the mathematical function $f$ described
in the preceding section.

As in Section \@ref(a-function-factory) above, we
develop our function a little bit at a time and then assemble all
the bits into the whole function.
And. as in that section, we need an object to be the argument of that
function to exercise our code bits on.
```{r map-rout-too}
names(rout.parents)
rout <- rout.parents[[1]][[1]]
rout <- rout.parents$GC[[1]]
```

Now we already have a lot of setup we can just repeat from Section
\@ref(a-function-factory) above.
```{r map-too-repeated-setup}
<<map-setup>>
<<map-subsampling>>
<<map-fake>>
<<map-extractors>>
<<map-which-ind>>
```

But we also need a bit more setup, following R function `map.factory.too`
in Section 12.3.2 of @zenodo.
```{r map-too-setup-ifit}
    alpha <- rout$alpha
    ifit <- which(names(alpha) == "fit")
    if (length(ifit) != 1)
        stop("no fixed effect named fit")
```

Now we can just copy the function returned by the map factory from
Section 12.3.2 of @zenodo.

Again we write the code in the body of the function before the function itself,
so we need an R object to serve as the function argument.
```{r map-too-balpha}
balpha <- c(0, alpha)
```

Then the function body is
```{r map-too-function-to-return}
        stopifnot(is.numeric(balpha))
        stopifnot(is.finite(balpha))
        stopifnot(length(balpha) == 1 + length(alpha))
        b <- balpha[1]
        alpha <- balpha[-1]
        alpha[ifit] <- alpha[ifit] + b
        xi <- predict(aout, newcoef = alpha,
            model.type = "conditional", is.always.parameter = TRUE)
        xi <- matrix(xi, ncol = nnode)
        # always use drop = FALSE unless you are sure you don't want that
        # here if we omit drop = FALSE and there is only one non-subsampling
        # node, the code will break (apply will give an error)
        xi <- xi[ , ! is.subsamp, drop = FALSE]
        mu <- apply(xi, 1, prod)
        # mu is unconditional mean values for model without subsampling
        # in this application all components mu are the same because no
        # covariates except varb, so just return only one
        mu[1]
```

Everything seems to be all right, so we put it all together.
```{r map.factory.other}
map.factory.other <- function(rout) {
<<map-setup>>
<<map-subsampling>>
<<map-fake>>
<<map-extractors>>
<<map-which-ind>>
<<map-too-setup-ifit>>
    # return map function
    function (balpha) {
<<map-too-function-to-return>>
    }
}
```

Try it out.
```{r map.too.try}
map.too <- map.factory.other(rout)
map.too(balpha)
```
### FFTNS Estimates

We use the function factory defined in the preceding section to make
FFTNS estimates.
```{r doit.fftns}
doit.fftns <- function(rout) {
    map.too <- map.factory.other(rout)
    balpha.hat <- c(0, rout$alpha)
    g <- grad(map.too, balpha.hat)
    dmu.db <- g[1]
    mu.hat <- map.too(balpha.hat)
    nu.hat <- rout$nu["parental"]
    as.numeric(dmu.db^2 * 4 * nu.hat / mu.hat)
}
```

Not too hard.  If it isn't clear what these computations are, compare
with Section 12.3.2 of @zenodo.

Try it.
```{r doit.fftns.one}
doit.fftns(rout)
```

Do all.
```{r doit.fftns.all}
lapply(rout.parents, function(x) lapply(x, doit.fftns))
```
Seems to agree with Table 1 of @zenodo.

### FFTNS Estimates with Standard Errors

Again following @zenodo, Section 12.3.2, we modify R function `doit.fftns`
to also compute standard errors.
```{r doit.fftns.se}
doit.fftns <- function(rout) {
    map.too <- map.factory.other(rout)
    balpha.hat <- c(0, rout$alpha)
    g <- grad(map.too, balpha.hat)
    h <- hessian(map.too, balpha.hat)
    dmu.db <- g[1]
    dmu.dalpha <- g[-1]
    d2mu.db.dalpha <- h[1, -1]
    mu.hat <- map.too(balpha.hat)
    nu.hat <- rout$nu["parental"]
    dfftns <- c(- 4 * nu.hat * dmu.dalpha * dmu.db^2 / mu.hat^2 +
        8 * nu.hat * dmu.db * d2mu.db.dalpha / mu.hat,
        4 * dmu.db^2 / mu.hat, 0)
    fishinv <- vcov(rout, standard.deviation = FALSE)
    fftns.se <- t(dfftns) %*% fishinv %*% dfftns
    fftns.se <- sqrt(as.vector(fftns.se))
    point.estimate <- dmu.db^2 * 4 * nu.hat / mu.hat
    c(estimate = point.estimate, std.err. = fftns.se)
}
```

Try it.
```{r doit.fftns.with.se.one}
doit.fftns(rout)
```

Do all.
```{r doit.fftns.with.se.all, cache=TRUE, dependson="reaster.parents"}
fftns.results <- lapply(rout.parents, function(x) lapply(x, doit.fftns))
fftns.results
```
Seems to agree with Table 1 of @zenodo except for the ones that were `NA`
there (which were incorrect).

### Wait, What?

But there still seems to be an issue with
KW in 2015, which is ridiculously large.

Check that (debug).
```{r debug}
rout <- rout.parents$KW[["2015"]]
foo <- vcov(rout, standard.deviation = FALSE)
foo
max(foo)
```

Nothing there.  Continue.
```{r debug.too, error=TRUE}
map.too <- map.factory.other(rout)
balpha.hat <- c(0, rout$alpha)
g <- grad(map.too, balpha.hat)
h <- hessian(map.too, balpha.hat)
dmu.db <- g[1] |> print()
dmu.dalpha <- g[-1] |> print()
d2mu.db.dalpha <- h[1, -1] |> print()
mu.hat <- map.too(balpha.hat) |> print()
nu.hat <- rout$nu["parental"] |> print()
```

It seems the results of R function `hessian` from R package `numDeriv`
are ridiculous.  We have observed similar problems before (not published).

### A Likelihood Interval

Hence we produce a likelihood interval, or more precisely
an objective-function-based interval, where the objective function in
question is the approximation to the log likelihood used
by R function `reaster`.

This uses R function `objfun.factory` which is new in R package `aster`
in version 1.3-4 which this document requires.
```{r objfun}
objfun <- with(rout, objfun.factory(fixed, random, response,
    obj$pred, obj$fam, as.vector(obj$root), zwz))
theta.hat <- with(rout, c(alpha, b, nu))
objfun(theta.hat)$value
```

This approximates minus log likelihood, so the likelihood ratio test statistic
is approximated by
```{r objfun.lrt}
lrt.factory <- function(objfun, theta.hat) {
    lrt.min <- objfun(theta.hat)$value
    function(theta) 2 * (objfun(theta)$value - lrt.min)
}
lrt <- lrt.factory(objfun, theta.hat)
```

We form a `r round((1 - 2 * pnorm(-1)) * 100, 2)`% confidence interval
by maximizing and minimizing a parameter of interest, in this case the FFTNS
prediction calculated by `map.too` over the region of the parameter space
where R function `lrt` has the value less than or equal to one.  We choose
this confidence interval to be equivalent to a Wald interval one standard
error to either side of the point estimate.  Of course, a likelihood interval
will not be symmetric about the point estimate.
(We will deal with that later.)

Now we make the objective function.  This is just the parameter
estimate evaluated by R function `map.too` except we change the value to
`Inf` ($+ \infty$) when we are minimizing and `-Inf` ($- \infty$) when
maximizing when we are off the constraint set, that is when R function `lrt`
evaluates to greater than 1.
```{r objfun-fftns}
objfun.fftns.factory <- function(rout, direction = c("minimize", "maximize")) {
    direction <- match.arg(direction)
    stopifnot(inherits(rout, "reaster"))
    map.too <- map.factory.other(rout)
    objfun <- with(rout, objfun.factory(fixed, random, response,
        obj$pred, obj$fam, as.vector(obj$root), zwz))
    theta.hat <- with(rout, c(alpha, b, nu))
    lrt.min <- objfun(theta.hat)$value
    lrt <- function(theta) 2 * (objfun(theta)$value - lrt.min)
    is.alpha <- seq_along(theta.hat) <= length(rout$alpha)
    is.nu <- c(rep(FALSE, length(rout$alpha) + length(rout$b)),
        names(rout$nu) == "parental")
    stopifnot(sum(is.nu) == 1)
    function(theta) {
        stopifnot(is.numeric(theta))
        stopifnot(is.finite(theta))
	stopifnot(length(theta) == length(theta.hat))
        if (lrt(theta) > 1) return(if (direction == "minimize") Inf else -Inf)
	alpha <- theta[is.alpha]
	balpha <- c(0, alpha)
	nu <- theta[is.nu]
        g <- grad(map.too, balpha)
        dmu.db <- g[1]
        mu.hat <- map.too(balpha)
        as.numeric(dmu.db^2 * 4 * nu / mu.hat)
    }
}
```

OK.  Minimize.
```{r fftns.lower,cache=TRUE}
oout.dn <- optim(theta.hat, objfun.fftns.factory(rout), method = "SANN",
    control = list(trace = TRUE, parscale = rep(1e-6, length(theta.hat))))
```
OK.  Maximize.
```{r fftns.upper,cache=TRUE}
oout.up <- optim(theta.hat, objfun.fftns.factory(rout), method = "SANN",
    control = list(trace = TRUE, parscale = rep(1e-6, length(theta.hat)),
        fnscale = -1))
```

Get our interval.
```{r fftns.interval}
c(oout.dn$value, oout.up$value)
```

If we redo, do we get a wider interval?
```{r fftns.redo, cache=TRUE}
oout.dn <- optim(oout.dn$par, objfun.fftns.factory(rout), method = "SANN",
    control = list(trace = TRUE, parscale = rep(1e-6, length(theta.hat))))
oout.up <- optim(oout.up$par, objfun.fftns.factory(rout), method = "SANN",
    control = list(trace = TRUE, parscale = rep(1e-6, length(theta.hat)),
        fnscale = -1))
c(oout.dn$value, oout.up$value)
```

Hardly changed.  So that's it.

Now that we have a method for making (approximate)
likelihood-based confidence intervals, perhaps we should use that instead
of standard errors and asymptotic normality.  But we won't change horses
in the middle of the stream.  So we are done.  Fix up (by hand) the
standard error that is ridiculous.
```{r fftns.fixup}
fftns.results$KW[["2015"]]
fftns.results$KW[["2015"]]["std.err."] <-
    diff(c(oout.dn$value, oout.up$value)) / 2
fftns.results$KW[["2015"]]
```

## New Table 3

Now we combine Table \@ref(tab:decomp) above, whose numbers are in R object
`fout`, with the numbers for FFTNS predictions that were computed in the
preceding section and originally were (mostly) in @zenodo.
```{r show.fout}
head(fout)
head(fftns.results)
```

First merge these two lists of lists.
```{r tab3.merge}
qux <- character(0)
quux <- character(0)
quuux <- double(0)
quuuux <- double(0)
for (site in names(fout)) {
    for (year in names(fout[[1]])) {
	prev.year <- as.character(as.numeric(year) - 1)
        lab <- paste0(site.translate[site], " ", prev.year, "-", year)
        foo <- fout[[site]][[year]]
        bar <- fftns.results[[site]][[year]]
	baz <- c(foo$estimates[-4], bar[1], foo$estimates[4])
	baze <- c(foo$std.err.[-4], bar[2], foo$std.err.[4])
	qux <- c(qux, rep(lab, 5))
	quux <- c(quux,
            c("total", "environmental", "selection", "fftns", "residual"))
	quuux <- c(quuux, baz)
	quuuux <- c(quuuux, baze)
    }
}
foo <- data.frame(Subset = qux, change = quux, estimate = quuux,
    std.err = quuuux)
```

Ready to make table.
```{r ft.align="center", tab.cap="Estimates of Change in Mean Fitness", label = "tab3"}
foot <- as.data.table(foo) |>
    as_grouped_data(groups = c("Subset"),
        columns = c("change", "estimate", "std.err")) |>
    flextable() |>
    set_header_labels(change = "Change", estimate = "Estimate",
        std.err = "Std. Error") |>
    colformat_double(digits = 4)
foot
```

OK.  Ship out to Microsoft Word.
```{r table.three.to.word}
foot <- set_caption(foot, "Estimates of Change in Mean Fitness")
save_as_docx(foot, path = "table3.docx")
```

# Plotting the Decomposition, Try Two

We make more plots and output them as separate PDF files for the paper.
This time no arrows.

## Six Different Plots

```{r plots.six}
crit <- qnorm(0.975)
crit
pdf("plots-six.pdf")
par(mar = c(3, 4, 4, 0) + 0.1)
par(mfrow = c(3, 2))
for (site in names(fout)) {
    for (year in names(fout[[1]])) {
	prev.year <- as.character(as.numeric(year) - 1)
        lab <- paste0(site.translate[site], " ", prev.year, "-", year)
        foo <- fout[[site]][[year]]
        errbar(1:4,
            foo$estimate,
            foo$estimate + crit * foo$std.err.,
            foo$estimate - crit * foo$std.err.,
	    main = lab, axes = FALSE,
	    ylab = "change in mean fitness", xlab = "",
	    xlim = c(0.5, 4.5))
	box()
	axis(side = 2)
	axis(side = 1, at = 1:4, tick = FALSE, labels = names(foo$estimate))
    abline(h = 0)
    }
}
```

# Making a Dataset for R Package Aster

Just the parental Grey Cloud Dunes 2015 data.
```{r dataset}
my_site <- "GC"
my_year <- "2015"
exists(site)
rm(year)
subdat <- subset(data.primary[[my_site]],
    year == my_year & cohort == "greenhouse")
subdat <- droplevels(subdat)
redata <- reshape(subdat, varying = list(vars), direction = "long",
    timevar = "varb", times = as.factor(vars), v.names = "resp")
redata <- transform(redata,
    fit = as.numeric(grepl("totalseeds", as.character(varb))),
    root = 1)
save(redata, file = "grey_cloud_2015.rda")
```

Check model fitting is OK.
```{r dataset.model.fit, cache=TRUE}
modmat.sire <- model.matrix(~ 0 + fit:paternalID, redata)
modmat.dam <- model.matrix(~ 0 + fit:maternalID, redata)
modmat.siredam <- cbind(modmat.sire, modmat.dam)
rout <- reaster(resp ~ fit + varb,
    list(parental = ~ 0 + modmat.siredam, block = ~ 0 + fit:block),
    pred, fam, varb, id, root, data = redata)
```

Check that the results are the same as before.
```{r dataset.check.results}
rout.other <- rout.parents[[my_site]][[my_year]]
all.equal(rout$alpha, rout.other$alpha)
all.equal(rout$nu, rout.other$nu)
all.equal(rout$b, rout.other$b)
```

# References

