---
title: "Estimates of Realized Response to Selection in *Chamaecrista fasciculata* and Decomposition into Environmental and Genetic Parts"
author:
  - "Mason W. Kulbaba^[St. Mary's University, mason.kulbaba@stmu.ca, https://orcid.org/0000-0003-0619-7089]"
  - "Seema N. Sheth^[Department of Plant and Microbial Biology, North Carolina State University, ssheth3@ncsu.edu, https://orcid.org/0000-0001-8284-7608]"
  - "Rachel E. Pain^[Ecology, Evolution and Behavior Graduate Program, University of Minnesota, repain@umn.edu]"
  - "Vincent M. Eckhart^[Department of Biology, Grinnell College, eckhart@grinnell.edu]"
  - "Charles J. Geyer^[School of Statistics, University of Minnesota, geyer@umn.edu, https://orcid.org/0000-0003-1471-1703]"
  - "Ruth G. Shaw^[Department of Ecology, Evolution and Behavior, University of Minnesota, shawx016@umn.edu, https://orcid.org/0000-0001-5980-9291]"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  bookdown::pdf_document2:
    extra_dependencies: "amscd"
    number_sections: true
    toc: true
    toc_depth: 3
linkcolor: blue
urlcolor: blue
bibliography: foo.bib
csl: journal-of-the-royal-statistical-society.csl
link-citations: true
---

# Abstract {-}

This work builds on @kulbaba-et-al and the correction to it [@zenodo]
to obtain estimates of the realized response to natural selection.
Those articles
presented estimates of mean fitness and additive genetic variance for fitness
for three populations of *Chamaecrista fasciculata*,
each grown in its home location in three years via aster analyses
of records of components of fitness for a pedigreed set of individuals.
Here, we consider the realized change in mean fitness from one generation
to the next, for comparison with the prediction from Fisher's
Fundamental Theorem of Natural Selection (FFTNS).
We divide change in mean fitness in one generation
into three parts:
that due to change in genetic composition described by FFTNS,
that due to change in genetic composition not described by FFTNS, and
that due to change in environment.
Here, we obtain estimates of a) mean fitness of the pedigreed parental
populations before selection (previously presented in @kulbaba-et-al and its
correction); b) mean fitness of the pedigreed parental population after
selection (i.e. accounting for the change in representation of the families
reflected in differential seed production); and mean fitness of the offspring
of the pedigreed sets
(i. e., the outcome of natural selection on the parental
generation when grown in the same sites in the following year).

We also obtain standard errors of our estimates.
In this we use a new scheme that treats random effects as parameters
to estimate because we do use estimates of random effects in our estimates
of mean fitness.

# License

This work is licensed under a Creative Commons
CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
(https://creativecommons.org/publicdomain/zero/1.0/).

The R markdown source for this document is the file `realized.Rmd`
in the GitHub private repository https://github.com/cjgeyer/mf
which will be made public whenever a paper based on it is submitted.

# R

 * The version of R used to make this document is `r getRversion()`.

 * The version of the `rmarkdown` package used to make this document is
   `r packageVersion("rmarkdown")`.

 * The version of the `bookdown` package used to make this document is
   `r packageVersion("bookdown")`.

 * The version of the `aster` package used to make this document is
   `r packageVersion("aster")`.

 * The version of the `numDeriv` package used to make this document is
   `r packageVersion("numDeriv")`.

 * The version of the `Matrix` package used to make this document is
   `r packageVersion("Matrix")`.

 * The version of the `parallel` package used to make this document is
   `r packageVersion("parallel")`.

 * The version of the `kableExtra` package used to make this document is
   `r packageVersion("kableExtra")`.

 * The version of the `Hmisc` package used to make this document is
   `r packageVersion("Hmisc")`.

Attach packages.
```{r package}
library("aster")
library("numDeriv")
library("Matrix")
library("parallel")
options("mc.cores" = detectCores())
library("kableExtra")
library("Hmisc")
```

Need at least version 1.3-4 of R package `aster` for R generic function `vcov`
to work on results of calls to R functions `aster` and `reaster`.
```{r aster-version}
stopifnot(compareVersion(as.character(packageVersion("aster")), "1.3-4") >= 0)
```
This version is now on CRAN so can be downloaded using R function
`install.packages` or the equivalent my mousing around in the menus of some app.

```{r checkerrors, echo=FALSE}
# make all code chunks after this one have option error=TRUE
# knitr::opts_chunk$set(error=TRUE)
```

# Data

## Files

```{r key.gc, echo=FALSE}
key.data <- tools::md5sum("mf.rda")
```

For the analyses here. the data files are
```{r data.input}
load("mf.rda")
ls()
sapply(data.primary, class)
```
for

 * Conard Environmental Research Area (CERA),

 * Grey Cloud Dunes Scientific and Natural Area, and

 * Kellogg-Weaver Dunes, also called McCarthy Lake,

respectively.  These files include the same data on the same individuals
as in the data files used by @kulbaba-et-al and @zenodo but also include
more individuals, who are offspring of those analyzed before.
For more details, see @kulbaba-et-al.

Much preprocessing of these data has already been done.  See
the file `fixup-data.pdf` in this repository.

## Structure

We do aster analyses with random effects (@aster2, @reaster) for
an aster model with graph
$$
\begin{CD}
  1 @>\text{Ber}>> \texttt{Germ}
  @>\text{Ber}>> \texttt{flw}
  @>\text{Poi}>> \texttt{total.pods}
  @>\text{samp}>> \texttt{total.pods.collected}
  @>\text{Poi}>> \texttt{totalseeds}
\end{CD}
$$
where the variables are

 * `Germ` is germination indicator (0 = no, 1 = yes), conditionally Bernoulli.

 * `flw` is survival to flowering (0 = no, 1 = yes),
    conditionally Bernoulli.

 * `total.pods` is total number of pods produced,
   conditionally Poisson.

 * `total.pods.collected` is number of pods collected,
   conditionally Bernoulli (i.e. each pod may be collected or not).
   The arrow leading to this node
   is a subsampling arrow.  The number of pods collected is
   a random sample of the pods produced.

 * `totalseeds` is total number of seeds counted from collected pods,
   conditionally Poisson.

As always with aster models, the name of the distribution for an arrow
is the name of the conditional distribution of the successor variable
given the predecessor variable.
The arrow labeled `samp` is a subsampling arrow.  It is a Bernoulli
arrow but the sampling is experimental rather than biological.
This arrow may be [missing in some analyses](#alternative-structure).

Set graphical model description in R.
```{r graph}
vars <- c("Germ", "flw", "total.pods", "total.pods.collected", "totalseeds")
pred <- c(0, 1, 2, 3, 4)
fam <- c(1, 1, 2, 1, 2)
```

## Alternative Structure

Get years.
```{r years}
years <- sort(unique(data.primary[[1]]$year))
names(years) <- years
years

years.offspring <- with(data.primary[[1]], year[cohort == "field"]) |>
    sort() |> unique()
names(years.offspring) <- years.offspring
years.offspring
```

And for each site-year combination, subset data for parents
(`cohort == "greenhouse"`) and check for presence of subsampling.
```{r check.subsampling.parents}
samp.parents <- lapply(data.primary, function(x) lapply(years, function(y) {
    subdat <- subset(x, year == y & cohort == "greenhouse")
    subdat <- droplevels(subdat)
    with(subdat, any(total.pods > total.pods.collected))
    }))
samp.parents
```
We see that we have two parental generation analyses (out of nine) in which
there is no subsampling.  For these we have to change the aster graph for
individuals to omit the subsampling arrow.
```{r graph.no.subsamp}
vars.no.samp <- c("Germ", "flw", "total.pods", "totalseeds")
pred.no.samp <- c(0, 1, 2, 3)
fam.no.samp <- c(1, 1, 2, 2)
```

Now do the same check for offspring
(`cohort == "field"`).
```{r check.subsampling.offspring}
samp.offspring <- lapply(data.primary, function(x)
    lapply(years.offspring, function(y) {
    subdat <- subset(x, year == y & cohort == "field")
    subdat <- droplevels(subdat)
    with(subdat, any(total.pods > total.pods.collected))
    }))
samp.offspring
```
So no issues with this for offspring: all offspring generation analyses
have subsampling.

# Analyses with R Function Reaster

## Parents

We have nine analyses to do here, one for each site-year combination.

We have three covariates: sire, dam, and block, which we make random effects.
We `cbind` the model matrices for sire and dam, so they share a variance
component.
```{r reaster.parents, cache=TRUE, cache.extra=key.data}
rout.parents <- mclapply(data.primary, function(x) mclapply(years, function(y) {
    subdat <- subset(x, year == y & cohort == "greenhouse")
    subdat <- droplevels(subdat)
    has.subsamp <- with(subdat, any(total.pods > total.pods.collected))
    if (has.subsamp) {
        redata <- reshape(subdat, varying = list(vars), direction = "long",
            timevar = "varb", times = as.factor(vars), v.names = "resp")
    } else {
        redata <- reshape(subdat, varying = list(vars.no.samp),
            direction = "long", timevar = "varb",
            times = as.factor(vars.no.samp), v.names = "resp")
    }
    redata <- transform(redata,
        fit = as.numeric(grepl("totalseeds", as.character(varb))),
        root = 1)
    modmat.sire <- model.matrix(~ 0 + fit:paternalID, redata)
    modmat.dam <- model.matrix(~ 0 + fit:maternalID, redata)
    modmat.siredam <- cbind(modmat.sire, modmat.dam)
    if (has.subsamp) {
        reaster(resp ~ fit + varb,
            list(parental = ~ 0 + modmat.siredam, block = ~ 0 + fit:block),
            pred, fam, varb, id, root, data = redata)
    } else {
        reaster(resp ~ fit + varb,
            list(parental = ~ 0 + modmat.siredam, block = ~ 0 + fit:block),
            pred.no.samp, fam.no.samp, varb, id, root, data = redata)
    }
    }, mc.preschedule = FALSE), mc.preschedule = FALSE)
```

Let's see what we got.
Check what we got.
```{r quid.est.happening}
lapply(rout.parents, function(x) sapply(x, function(x) inherits(x, "reaster")))
```

Any variance components estimated to be zero?
```{r quid.est.happening.too}
lapply(rout.parents, function(x) sapply(x, function(x) x$nu))
```
One case where block effects are estimated to be zero,
but all cases have parental effects estimated to be nonzero,
which is what we want.

## Offspring

We have six analyses to do here, one for each site-year combination
(in which there are offspring).
```{r reaster.offspring, cache=TRUE, cache.extra=key.data}
rout.offspring <- mclapply(data.primary,
    function(x) mclapply(years.offspring, function(y) {
    subdat <- subset(x, year == y & cohort == "field")
    subdat <- droplevels(subdat)
    redata <- reshape(subdat, varying = list(vars), direction = "long",
        timevar = "varb", times = as.factor(vars), v.names = "resp")
    redata <- transform(redata,
        fit = as.numeric(grepl("totalseeds", as.character(varb))),
        root = 1)
    reaster(resp ~ fit + varb,
        list(parental = ~ 0 + fit:grandpaternalID, block = ~ 0 + fit:block),
        pred, fam, varb, id, root, data = redata)
    }))
```

Let's see what we got.
Check what we got.
```{r quid.est.happening.offspring}
lapply(rout.offspring, function(x)
    sapply(x, function(x) inherits(x, "reaster")))
```

Any variance components estimated to be zero?
```{r quid.est.happening.too.offspring}
lapply(rout.offspring, function(x) sapply(x, function(x) x$nu))
```
No case where any variance components are estimated to be zero.

# Asymptotic Variance-Covariance Matrices of Estimates

## Parents

```{r vcov.parents, cache=TRUE, dependson="reaster.parents"}
vcov.parents <- mclapply(rout.parents, function(x)
    mclapply(x, vcov, complete = TRUE, re.too = TRUE,
    standard.deviation = FALSE))
```

## Offspring

```{r vcov.offspring, cache=TRUE, dependson="reaster.offspring"}
vcov.offspring <- mclapply(rout.offspring, function(x)
    mclapply(x, vcov, complete = TRUE, re.too = TRUE,
    standard.deviation = FALSE))
```

# Mapping Sire and Grandsire Effects to Mean Values

## A Function Factory

We follow Section 9 of @zenodo *mutatis mutandis*.  The main changes are
here we will have a vectorizing function that simultaneously does
mean fitness values for a specified set of individuals.

We follow the dictates of literate programming
([Wikipedia page](https://en.wikipedia.org/wiki/Literate_programming))
developing our function a little bit at a time and then assembling all
the bits into the whole function.  The function we are developing will have
one argument `rout`, so we make an object of the correct type to exercise
our code bits on.
```{r map-rout}
rout <- rout.parents[[1]][[1]]
```

Our first bit of code does some error checking and setup.
```{r map-setup}
    stopifnot(inherits(rout, "reaster"))
    aout <- rout$obj
    stopifnot(inherits(aout, "aster"))
    nnode <- ncol(aout$x)
    nind <- nrow(aout$x)
    fixed <- rout$fixed
    random <- rout$random
```

The next job is to figure out the subsampling nodes of the graph (if
present).  Here the answer is the fourth of five.
```{r map-subsampling}
    if (nnode == 4) {
        is.subsamp <- rep(FALSE, 4)
    } else if (nnode == 5) {
        is.subsamp <- c(FALSE, FALSE, FALSE, TRUE, FALSE)
    } else stop("can only deal with graphs for individuals with 4 or 5 nodes",
        "\nand graph is linear, and subsampling arrow is 4th of 5")
```

Now we have to deal with the problem that the only R function we have that
maps from canonical to mean value parameters is R function `predict.aster`
and we need to give it an object of class `"aster"`.  So we fake one.
Here we only want the random effects that have the string `"paternalID"`
in their names (that is, either paternalID or grandpaternalID).
```{r map-fake}
    # fake object of class aster
    randlab <- unlist(lapply(rout$random, colnames))
    include.random <- grepl("paternalID", randlab, fixed = TRUE)
    fake.out <- aout
    fake.beta <- with(rout, c(alpha, b[include.random]))
    modmat.random <- Reduce(cbind, random)
    stopifnot(ncol(modmat.random) == length(rout$b))
    # never forget drop = FALSE in programming R
    modmat.random <- modmat.random[ , include.random, drop = FALSE]
    fake.modmat <- cbind(fixed, modmat.random)
    # now have to deal with objects of class aster (as opposed to reaster)
    # thinking model matrices are three-way arrays.
    stopifnot(prod(dim(aout$modmat)[1:2]) == nrow(fake.modmat))
    fake.modmat <- array(as.vector(fake.modmat),
        dim = c(dim(aout$modmat)[1:2], ncol(fake.modmat)))
    fake.out$modmat <- fake.modmat
```

The next job is to figure out which parameters are which.
```{r map-extractors}
    nparm <- length(rout$alpha) + length(rout$b) + length(rout$nu)
    is.alpha <- 1:nparm %in% seq_along(rout$alpha)
    is.bee <- 1:nparm %in% (length(rout$alpha) + seq_along(rout$b))
    is.nu <- (! (is.alpha | is.bee))
```

The next job is to figure out which individuals are in which "families"
(paternalID) and to get one of each for the output vector.
```{r map-which-ind}
    # figure out individuals from each family
    m <- rout$random$parental
    dads <- grep("paternal", colnames(m))
    # get family, that is, paternalID or grandpaternalID as the case may be
    fams <- colnames(m)[dads] |> sub("^.*ID", "", x = _)
    # drop maternal effects columns (if any)
    m.dads <- m[ , dads, drop = FALSE]
    # make into 3-dimensional array, like obj$modmat
    m.dads <- array(m.dads, c(nind, nnode, ncol(m.dads)))
    # only keep fitness node
    # only works for linear graph
    m.dads <- m.dads[ , nnode, ]
    # redefine dads as families of individuals
    stopifnot(as.vector(m.dads) %in% c(0, 1))
    stopifnot(rowSums(m.dads) == 1)
    # tricky, only works because each row of m.dads
    # is indicator vector of family,
    # so we are multiplying family number by zero or one
    dads <- drop(m.dads %*% as.integer(fams))
    # find one individual in each family
    sudads <- sort(unique(dads))
    which.ind <- match(sudads, dads)
```

That finishes the setup.  Now we write the result of our factory function,
which is another function with a single argument `alphabeenu` which is the
vector of all the variables (fixed effects, random effects, and variance
components.
```{r map-alphabeenu}
alphabeenu <- with(rout, c(alpha, b, nu))
```

So first check that this argument is OK.
```{r map-return-check}
        stopifnot(is.numeric(alphabeenu))
        stopifnot(is.finite(alphabeenu))
        stopifnot(length(alphabeenu) == nparm)
```
Then extract the various parts.  And make the `coefficients` component
of our fake object of class `"aster"` to be the fixed effects vector `alpha`
plus the random effects vector of the random effects we are "predicting"
(those indicated by the indicator vector `include.random`).
```{r map-return-extract}
        alpha <- alphabeenu[is.alpha]
        bee <- alphabeenu[is.bee]
        nu <- alphabeenu[is.nu]
        fake.beta <- c(alpha, bee[include.random])
        fake.out$coefficients <- fake.beta
```

Do the prediction, mapping unconditional submodel canonical parameters $\beta$
to conditional mean value parameters $\xi$.
```{r map-return-predict}
        pout <- predict(fake.out, model.type = "conditional",
            is.always.parameter = TRUE)
```

Then put the result $\xi$ into matrix form, rows are individuals, columns
are nodes of the graph for individuals.  Toss the column of subsampling nodes
(if any) and multiply the rest to get unconditional mean value parameters for
fitness (mean fitness).  Put in a comment that this only works for this
particular graph.
```{r map-return-moo}
        xi <- matrix(pout, ncol = nnode)
        xi <- xi[ , ! is.subsamp, drop = FALSE]
        mu <- apply(xi, 1, prod)
```

Finally, we put names on `mu` and return only one for each "family"
(paternalID).
```{r map-return-subset}
        mu <- mu[which.ind]
        names(mu) <- paste0("PID",
            formatC(sudads, format="d", width=3, flag="0"))
```

Now put it all together in a function factory.
```{r map}
map.factory <- function(rout) {
<<map-setup>>
<<map-subsampling>>
<<map-fake>>
<<map-extractors>>
<<map-which-ind>>
    function(alphabeenu) {
<<map-return-check>>
<<map-return-extract>>
<<map-return-predict>>
<<map-return-moo>>
<<map-return-subset>>
        return(mu)
    }
}
```

R function `map.factory` has one argument, an object of class `reaster`
produced by a call to R function `reaster` and produces a function with
one argument, which is a vector $\theta = (\alpha, c, \sigma)$ that contains
the variables (fixed effects, standardized random effects, and standard
deviation components), and that produced function returns
mean fitness estimates for one individual from each "family" (sire for
parental fits, grandsire for offspring fits).
That is, it is a function whose value is another function.

**Warning:** This function only works for linear aster graphs for "individuals"
(in scare quotes) having four or five arrows and the only subsampling is the
fourth of five.

Try it out.
```{r fitness-try-out, error=TRUE}
map <- map.factory(rout)
map(alphabeenu)
```

## Apply To Parents

In hindsight, we did not want to apply R generic function `vcov` as
we did above to make a separate list.  So we do it again here.
```{r moo.parents, cache=TRUE, dependson="reaster.parents"}
moo.parents <- mclapply(rout.parents, function(x) mclapply(x, function(y) {
    map <- map.factory(y)
    alphabeenu <- with(y, c(alpha, b, nu))
    estimates <- map(alphabeenu)
    jack <- jacobian(map, alphabeenu)
    vcov.base <- vcov(y, complete = TRUE, re.too = TRUE,
        standard.deviation = FALSE)
    vcov.estimates <- jack %*% vcov.base %*% t(jack)
    return(list(estimates = estimates, vcov = vcov.estimates))
}))
```

## Apply To Offspring

Same for offspring.
```{r moo.offspring, cache=TRUE, dependson="reaster.offspring"}
moo.offspring <- mclapply(rout.offspring, function(x) mclapply(x, function(y) {
    map <- map.factory(y)
    alphabeenu <- with(y, c(alpha, b, nu))
    estimates <- map(alphabeenu)
    jack <- jacobian(map, alphabeenu)
    vcov.base <- vcov(y, complete = TRUE, re.too = TRUE,
        standard.deviation = FALSE)
    vcov.estimates <- jack %*% vcov.base %*% t(jack)
    return(list(estimates = estimates, vcov = vcov.estimates))
}))
```

# Change in Mean Fitness Due to Selection

This is one of the main quantities of scientific interest, and the others
are related to this.  For each vector $\mu$ returned by R function `map`
we turn it into *relative fitness* by dividing by the mean.
This is fitness "after selection but before reproduction".
We compare this with fitness
before selection, which is equal across families.  So the estimate of interest
is `mean(mu * (mu / mean(mu) - 1))`

We make this into a function so we can differentiate it numerically.
```{r change}
fitness_change <- function(mu) mean(mu * (mu / mean(mu) - 1))
```

Now we apply this function to parents and offspring.
```{r change.parents, cache=TRUE, dependson="moo.parents"}
change.parents <- mclapply(moo.parents, function(x) mclapply(x, function(y) {
    grad <- grad(fitness_change, y$estimates)
    my.vcov <- t(grad) %*% y$vcov %*% grad
    return(list(estimates = fitness_change(y$estimates), vcov = my.vcov))
}))
```
```{r change.offspring, cache=TRUE, dependson="moo.offspring"}
change.offspring <- mclapply(moo.offspring, function(x)
    mclapply(x, function(y) {
    grad <- grad(fitness_change, y$estimates)
    my.vcov <- t(grad) %*% y$vcov %*% grad
    return(list(estimates = fitness_change(y$estimates), vcov = my.vcov))
}))
```

Turn these into tables
```{r change-tables}
doit <- function(w, cap, lab) {
    e <- lapply(w, function(x) lapply(x, function(y) y$estimates)) |> unlist()
    se <- lapply(w, function(x) lapply(x, function(y) y$vcov)) |> unlist() |>
        sqrt()
    foo <- cbind(estimates = e, std.err. = se)
    site <- substr(rownames(foo), 1, 2)
    site.cs <- match("CS", site)
    site.gc <- match("GC", site)
    site.kw <- match("KW", site)
    year <- substr(rownames(foo), 4, 7)
    rownames(foo) <- year
    kbl(foo, caption = cap, label = lab,
        format = "latex", escape = FALSE, booktabs = TRUE, digits = 4) |>
        kable_styling() |>
        pack_rows("Grey Cloud Dunes", min(site.gc), max(site.gc),
            indent = FALSE) |>
        pack_rows("McCarthy Lake", min(site.kw), max(site.kw),
            indent = FALSE) |>
        pack_rows("CERA", min(site.cs), max(site.cs), indent = FALSE)
}
doit(change.parents, "Change in Mean Fitness Due to Selection in Parents",
    "changeParents")
doit(change.offspring, "Change in Mean Fitness Due to Selection in Offspring",
    "changeOffspring")
```

# Comparison of Mean Fitness in Parents and Offspring

Now we want to make inter-dataset comparisons and inter-year comparisons.
To continue with functional programming we need to temporarily step outside
of functional programming and use some plain old for loops to re-assemble
the data the way we want it.  All of our comparisons in this section will
involve "parents" in one year and "parents" in the following year (these
are pedigreed individuals, and individuals planted in one year have full
sibs planted in the next.  The "offspring" are open pollinated individuals
(mothers known, fathers unknown) whose mothers are individuals among the
"parents" in the previous year.

We collect each of these into one vector with one variance-covariance
matrix so we can apply one function to them to get estimates and differentiate
that one function numerically to apply the delta method to get standard
errors.
```{r re-arrange,cache=TRUE,dependson=c("change.parents","change.offspring")}
redata <- list()
for (site in names(moo.offspring)) {
    for (year in names(moo.offspring[[site]])) {
        year.before <- as.character(as.numeric(year) - 1)
        moo.prev <- moo.parents[[site]][[year.before]]
        moo.par <- moo.parents[[site]][[year]]
        stopifnot(names(moo.prev$estimates) == names(moo.par$estimates))
        moo.off <- moo.offspring[[site]][[year]]
        stopifnot(names(moo.off$estimates) %in% names(moo.prev$estimates))
        idx <- match(names(moo.off$estimates), names(moo.prev$estimates))
        my.off.est <- rep(0, length(moo.prev$estimates))
        my.off.vcov <- matrix(0, length(my.off.est), length(my.off.est))
        my.off.est[idx] <- moo.off$estimates
        my.off.vcov[idx, idx] <- moo.off$vcov
        redata[[site]][[year]] <- list( estimates =
            c(moo.prev$estimates, moo.par$estimates, my.off.est),
            vcov = bdiag(moo.prev$vcov, moo.par$vcov, my.off.vcov))
    }
}
```

Here is our function to compute estimates of interest.

  * total change in mean fitness over two years
    \begin{equation}
    \bar{\mu}_\text{total} =
    \text{mean}\left(\mu_\text{off} \cdot
    \frac{\mu_\text{prev}}{\text{mean}(\mu_\text{prev})}\right)
    - \text{mean}(\mu_\text{prev})
    \end{equation}
    offspring mean fitness weighted by relative fitness of their parent
    (because in the experiment offspring were not planted according to
    their frequency, so we must correct for that)

and this is divided into

  * change due to selection (what Fisher's fundamental theorem of
    natural selection is supposed to predict) in the parents
    \begin{equation}
    \bar{\mu}_\text{selection} =
    \text{mean}\left[\left(\frac{\mu_\text{prev}}{\text{mean}(\mu_\text{prev})}
    - 1\right) \cdot \mu_\text{prev} \right]
    \end{equation}
    parental mean fitness weighted by relative fitness minus one (this all
    takes place in the parental sample).

  * environmental change
    \begin{equation}
    \bar{\mu}_\text{environment} =
    \text{mean}(\mu_\text{par}) - \text{mean}(\mu_\text{prev})
    \end{equation}
    difference between pedigreed individuals (which are full sibs) in different
    years.  (Of course, this includes some genetic change because full sibs
    are not clones.  But it does directly estimate the change due to different
    environments in the two years.)

  * residual change, the part of the total change not due to environment
    or selection
    \begin{equation}
    \bar{\mu}_\text{residual} =
    \bar{\mu}_\text{total} -
    \bar{\mu}_\text{selection} -
    \bar{\mu}_\text{environment}
    \end{equation}

```{r foo}
foo <- function(theta) {
    stopifnot(is.atomic(theta))
    stopifnot(is.vector(theta))
    stopifnot(is.numeric(theta))
    stopifnot(is.finite(theta))
    stopifnot(length(theta) %% 3 == 0)
    nmu <- length(theta) %/% 3
    is.prev <- seq_along(theta) <= nmu
    is.off <- seq_along(theta) > 2 * nmu
    is.par <- (! (is.prev | is.off))
    mu.prev <- theta[is.prev]
    mu.par <- theta[is.par]
    mu.off <- theta[is.off]
    
    delta.total <- sum(mu.off * mu.prev) / sum(mu.prev) - mean(mu.prev)
    delta.environ <- mean(mu.par - mu.prev)
    delta.fftns <- mean((mu.prev / mean(mu.prev) - 1) * mu.prev)
    delta.non.fftns <- delta.total - delta.environ - delta.fftns
    c(total = delta.total, environmental = delta.environ,
        selection = delta.fftns, residual = delta.non.fftns)
}
```

Try it out.
```{r foo.try}
e <- redata[[1]][[1]]$estimates
foo(e)
```

So now we want to use this function to calculate estimates and standard
errors.
```{r foo.doit,cache=TRUE,dependson="re-arrange"}
fout <- mclapply(redata, function(x) mclapply(x, function(y) {
    e <- foo(y$estimates)
    jack <- jacobian(foo, y$estimates)
    my.vcov <- jack %*% y$vcov %*% t(jack)
    list(estimates = e, std.err. = sqrt(as.vector(diag(my.vcov))))
}))
```

Look at one, just to see we are OK.
```{r foo.doit.look}
fout[[1]][[2]]
```

Now make a table for this.
```{r footab}
doit <- function(w) {
    e <- lapply(w, function(x) lapply(x, function(y) y$estimates)) |> unlist()
    se <- lapply(w, function(x) lapply(x, function(y) y$std.err.)) |> unlist()
    foo <- cbind(estimates = e, std.err. = se)
    site <- substr(rownames(foo), 1, 2)
    site.cs <- "CS" == site
    site.gc <- "GC" == site
    site.kw <- "KW" == site
    year <- substr(rownames(foo), 4, 7)
    year.2016 <- "2016" == year
    year.2017 <- "2017" == year
    kind <- substr(rownames(foo), 9, 100)
    rownames(foo) <- kind
    kbl(foo, caption = "Estimates of Change in Mean Fitness", label = "decomp",
        format = "latex", escape = FALSE, booktabs = TRUE, digits = 4) |>
        kable_styling() |>
        pack_rows("Grey Cloud Dunes, 2015-2016",
            min(which(site.gc & year.2016)),
            max(which(site.gc & year.2016)),
            indent = FALSE) |>
        pack_rows("Grey Cloud Dunes, 2016-2017",
            min(which(site.gc & year.2017)),
            max(which(site.gc & year.2017)),
            indent = FALSE) |>
        pack_rows("McCarthy Lake, 2015-2016",
            min(which(site.kw & year.2016)),
            max(which(site.kw & year.2016)),
            indent = FALSE) |>
        pack_rows("McCarthy Lake, 2016-2017",
            min(which(site.kw & year.2017)),
            max(which(site.kw & year.2017)),
            indent = FALSE) |>
        pack_rows("CERA, 2015-2016",
            min(which(site.cs & year.2016)),
            max(which(site.cs & year.2016)),
            indent = FALSE) |>
        pack_rows("CERA, 2016-2017",
            min(which(site.cs & year.2017)),
            max(which(site.cs & year.2017)),
            indent = FALSE)
}
doit(fout)
```

# Plotting the Decomposition

We are going to try to show these numbers and standard errors in a plot.
We will just do one example.
```{r plot-site-year}
site <- "GC"
year <- "2016"
year.before <- as.character(as.numeric(year) - 1)
f <- fout[[site]][[year]]
names(f)
```

Critical value.
```{r plot-crit}
conf.level <- 0.95
crit <- qnorm((1 + conf.level) / 2)
```

The horizontal length of the arrows in this plot are entirely meaningless.
They have to be different because we don't want the error bars to overlap.
Table \@ref(tab:decomp)
```{r plot-caption}
cap <- paste0("Estimates in Table 3 with Error Bars.",
   "  Horizontal coordinate is meaningless, but short vectors add up",
   " to long vector.",
   "  Vertical bars are approximate 95% confidence intervals.",
   "  Site is ", site, ".",
   "  Total is difference of parents (", year.before, ") and offspring (",
   year, ").",
   "  Selection is difference due to selection in parents (", year.before, ").",
   "  Environmental is difference due to different environments in the",
   " two years: difference of parents (", year.before, ") and full sibs grown",
   " along with offspring.",
   "  Residual is Total - Selection - Environmental, mostly genetic change",
   " not due to selection in parents or gene-environment interaction.")
```
```{r plot, fig.align="center", fig.cap=cap}
par(mar = c(1, 4, 0, 0) + 0.1)
xlength <- c(3, 0.9, 1.0, 1.1)
errbar(x = xlength, y = f$estimates,
   yplus = f$estimates + crit * f$std.err.,
   yminus = f$estimates - crit * f$std.err.,
   axes = FALSE, xlim = c(0, 3),
   ylab = "change in mean fitness",
   pch = NA_integer_)
box()
axis(side = 2)
arrows(x0 = rep(0, 4), x1 = xlength, y0 = rep(0, 4),
   y1 = f$estimates)

# now have to do extraordinarily painful calculation of angles to rotate text
par.pin <- par("pin")
par.plt <- par("plt")
par.usr <- par("usr")
par.plt <- c(diff(par.plt[1:2]), diff(par.plt[3:4]))
par.usr <- c(diff(par.usr[1:2]), diff(par.usr[3:4]))
foo <- par.pin * par.plt / par.usr
foo
angles <- atan(f$estimates / xlength * foo[2] / foo[1]) / pi * 180
angles
radians <- 2 * pi * angles / 360
cos(radians)
sin(radians)
delta.text <- 0.2
x <- xlength
y <- f$estimates
n <- names(y)
for (i in seq_along(f$estimates))
    text(x = x[i] / 2 - sin(radians[i]) * delta.text / foo[1],
    y = y[i] / 2 + cos(radians[i]) * delta.text / foo[2],
    labels = n[i], col = "darkgray", srt = angles[i])
```

# References

